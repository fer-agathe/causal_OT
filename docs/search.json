[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference with Counterfactuals and Optimal Transport",
    "section": "",
    "text": "1 Preface\n\nThis ebook contains the codes and explanations of codes used to produce the results of our paper titled Causal Inference with Counterfactuals and Optimal Transport.\n[TBD]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "transp-categ.html",
    "href": "transp-categ.html",
    "title": "3  Counterfactuals for Categorical Features",
    "section": "",
    "text": "3.1 1-to-1 Matching\n\\[\n\\definecolor{wongBlack}{RGB}{0,0,0}\n\\definecolor{wongGold}{RGB}{230, 159, 0}\n\\definecolor{wongLightBlue}{RGB}{86, 180, 233}\n\\definecolor{wongGreen}{RGB}{0, 158, 115}\n\\definecolor{wongYellow}{RGB}{240, 228, 66}\n\\definecolor{wongBlue}{RGB}{0, 114, 178}\n\\definecolor{wongOrange}{RGB}{213, 94, 0}\n\\definecolor{wongPurple}{RGB}{204, 121, 167}\n\\definecolor{colA}{RGB}{255, 221, 85}\n\\definecolor{colB}{RGB}{148, 78, 223}\n\\definecolor{colC}{RGB}{63, 179, 178}\n\\definecolor{colGpeZero}{RGB}{127, 23, 14}\n\\definecolor{colGpeUn}{RGB}{27, 149, 224}\n\\]\nConsider a categorical variables \\(x \\in \\{{\\color{colA}A}, {\\color{colB}B}, {\\color{colC}C}\\}\\), with group-specific distributions. The categorical variable could be, for example, the treatment administered for a disease: A=surgery, B=medication, C=no treatment. We consider two groups (this could non-Black/Black, for example). We want to build a counterfactual category for each individual from a group to the other.\nLet Group 1 and Group 0 represent two subpopulations in which the distribution of \\(x\\) differs:\nGroup 1 could be, for example, Black individuals, whereas Group 0 could be individuals who are not Black.\nAssume that the objective is to define, for each individual in Group 1, a counterfactual category that reflects the distributional characteristics of Group 0. That is, we want to know what would be the medical treatment of a Black individuals with a given treatment (e.g., “C=no treatment) had they been non-Black.\nLet us generate a dummy data set with 100 individuals in both Group 0 and Group 1.\nA way to obtain the counterfactual for the categorical variable is to implement a 1-to-1 matching procedure.\nEach category is assigned an arbitrary numeric value (e.g., \\(A = 1\\), \\(B = 2\\), \\(C = 3\\)), allowing us to define a cost matrix based on the absolute difference between encoded categories. That is, the cost of matching an individual from Group 1 with category \\(x_{j1}\\) to an individual from Group 0 with category \\(x_{i0}\\) is given by \\(C_{ij} = |x_{i0} - x_{j1}|\\).\nA linear sum assignment problem can then be used to find the matching that minimizes the total cost across pairs.\nThe matched category from Group 0 is interpreted as the counterfactual category for the corresponding Group 1 individual.\nWe can compute the distance between the observations from Group 0 to Group 1, by setting numeric values to each category: A=1, B=2, C=3:\nx0_index &lt;- match(x0, cat_levels)\nx1_index &lt;- match(x1, cat_levels)\ncost_matrix &lt;- outer(x0_index, x1_index, function(i, j) abs(i - j))\nThe linear sum assignment problem is tackled with solve_LSAP() from {clue}.\nlibrary(clue)\nassignment &lt;- solve_LSAP(cost_matrix)\nThe mapping can be stored in a tibble.\ntb_coupling &lt;- tibble(\n  x0 = x0,\n  x1 = x1[assignment]\n) |&gt; \n  mutate(\n    cost = abs(match(x0, cat_levels) - match(x1, cat_levels))\n  )\nWe compute the number of observations from Group 1 matched with observations from Group 0 per category.\ntb_coupling |&gt; \n  group_by(x1, x0) |&gt; \n  count() |&gt; \n  group_by(x1) |&gt; \n  mutate(prop = 100 * n / sum(n))\n\n# A tibble: 5 × 4\n# Groups:   x1 [3]\n  x1    x0        n  prop\n  &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 A     A        10   100\n2 B     A        20    40\n3 B     B        30    60\n4 C     A        20    50\n5 C     C        20    50\nThen, we can visualize the results, using an alluvial plot (Figure 3.1). All individuals in Group 1 with label A are matched directly to individuals in Group 0 with the same label. The remaining 40% of individuals in Group 0 labeled A are then matched to Group 1 individuals who originally had label B or C. This matching is performed probabilistically: among Group 1 individuals with label B, 60% retain their original label, and 40% are reclassified as A; among those with label C, 50% remain C, and 50% are reclassified as A.\nCodes to create the Figure.\nflow &lt;- data.frame(\n  depart = rep(LETTERS[1:3], 3),\n  category = rep(LETTERS[1:3], each = 3),\n  freq = as.vector(table(tb_coupling$x1, tb_coupling$x0))\n)\np_1 &lt;- ggplot(\n  data = flow,\n  mapping = aes(axis1 = depart, axis2 = category, y = freq)\n) +\n  geom_alluvium(aes(fill = category)) +\n  geom_stratum() +\n  scale_fill_manual(values = col_categ) +\n  geom_text(\n    stat = \"stratum\",\n    mapping = aes(label = after_stat(stratum)),\n    family = font_family\n  ) +\n  # scale_x_discrete(\n  #   limits = c(\"Group 1\", \"Group 0\"),\n  #   expand = c(0.15, 0.05)\n  # ) +\n  scale_x_discrete(\n    limits = c(\"Group 1\", \"Group 0\"),\n    labels = c(\n      \"Group 1\" = str_c(\"&lt;span style='color:\", col_group[2], \";'&gt;Group 1&lt;/span&gt;\"),\n      \"Group 0\" = str_c(\"&lt;span style='color:\", col_group[1], \";'&gt;Group 0&lt;/span&gt;\")\n    ),\n    expand = c(0.15, 0.05)\n  ) +\n  ylab(\"proportions\") + \n  scale_y_continuous(transform = ) +\n  # theme_minimal(base_size = font_size, base_family = font_family) +\n  theme_paper() +\n  theme(\n    axis.text.x = ggtext::element_markdown()\n  )\np_1\n\n\n\n\n\nFigure 3.1: Matching individuals given a categorical variable.\nThis can also be visualized on a ternary plot (Figure 3.2).\nCodes to create the Figure.\n# Create interpolated values using McCann (1997) displacement\nf_line_simplex &lt;- function(x, \n                           y, \n                           lgt = 601) {\n  \n  zx &lt;- as.numeric(clr(x))[1:2]\n  zy &lt;- as.numeric(clr(y))[1:2]\n  t &lt;- seq(0, 1, length = lgt)\n  \n  tx &lt;- cbind(\n    (1 - t) * zx[1] + t * zy[1], \n    (1 - t) * zx[2] + t * zy[2]\n  )\n  tx &lt;- cbind(tx, -(tx[, 1] + tx[, 2]))\n  df &lt;- as.data.frame(matrix(as.numeric(clrInv(tx)), lgt, 3))\n  names(df) &lt;- c(\"A\",\"B\",\"C\")\n  \n  df\n                           }\n\n# dummy dataset to create an empty ternary plot\nSB &lt;- tibble(\n  A = c(0.2, 0.3, 0.5, 0.6),\n  B = c(0.3, 0.4, 0.2, 0.1),\n  C = 1 - c(0.2, 0.3, 0.5, 0.6) - c(0.3, 0.4, 0.2, 0.1),\n  group = c(\"1\", \"1\", \"0\", \"0\")\n)\n\np_2 &lt;- ggtern(data = SB, aes(x = A, y = B, z = C)) +\n  # fake (invisible) points\n  # geom_point(size = 0.01, alpha = 0, aes(color = group)) +\n  # fake (invisible) lines\n  # geom_path(aes(color = group), data = SB, alpha = 0, show.legend = TRUE) +\n  scale_colour_manual(name = \"group\", values = col_group) +\n  guides(\n    colour = guide_legend(\n      override.aes = list(\n        linetype = \"solid\",\n        shape = NA,\n        size = 1.5,\n        alpha = 1\n      )\n    )\n  ) +\n  theme_light(base_size = font_size, base_family = font_family) +\n  theme_ggtern_paper() +\n  theme(\n    legend.title = element_text(size = font_size),\n    legend.text = element_text(size = font_size)\n    # tern.axis.hshift = .10\n  ) +\n  theme_latex(TRUE) +\n  theme_hidetitles()\n\n\np_2 &lt;- p_2 + \n  geom_text(mapping = aes(x = 0.9, y = 0.06, z = 0.08), label = p0[1], color = col_group[1], family = font_family, size = font_size-3, size.unit = \"pt\") +\n  geom_text(mapping = aes(x = 0.09, y = 0.9, z = 0.09), label = p0[2], color = col_group[1], family = font_family, size = font_size-3, size.unit = \"pt\") +\n  geom_text(mapping = aes(x = 0.08, y = 0.06, z = 0.9), label = p0[3], color = col_group[1], family = font_family, size = font_size-3, size.unit = \"pt\") + \n  geom_text(mapping = aes(x = 0.3, y = 0.1, z = 0.11), label = p1[1], color = col_group[2], family = font_family, size = font_size-3, size.unit = \"pt\") +\n  geom_text(mapping = aes(x = 0.15, y = 0.65, z = 0.25), label = p1[2], color = col_group[2], family = font_family, size = font_size-3, size.unit = \"pt\") +\n  geom_text(mapping = aes(x = 0.1, y = 0.2, z = 0.8), label = p1[3], color = col_group[2], family = font_family, size = font_size-3, size.unit = \"pt\") \n\n\nLi1 &lt;- f_line_simplex(x = c(.75, .125, .125), y = c(.125, .125, .75), lgt = 2)\nLi2 &lt;- f_line_simplex(x = c(.75, .125, .125), y = c(.125, .75, .125), lgt = 2)\np_2 &lt;- p_2 + \n  geom_line(\n    data = Li2, aes(x = A, y = B, z = C), \n    color = col_group[2], linwidth = .6,\n    arrow = arrow(length=unit(0.20,\"cm\"))\n  ) + \n  geom_line(\n    data = Li1, aes(x = A, y = B, z = C), \n    color = col_group[2], linwidth = .6,\n    arrow = arrow(length=unit(0.20,\"cm\"))\n  ) \np_2\n\n\n\n\n\nFigure 3.2: Matching individuals given a categorical variable, on a Ternary plot.\nCodes to export the figures in PDF.\np_matching_indiv &lt;- cowplot::plot_grid(\n  ggplotGrob(\n    p_1 +\n      # Remove top/bottom margin\n      theme(\n        plot.margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0)\n      )\n  ),\n  # table_grob,\n  ggplotGrob(\n    p_2 +\n      # Remove top/bottom margin\n      theme(\n        plot.background = element_rect(fill = \"transparent\", color = NA),\n        plot.margin = ggplot2::margin(t = 0, r = 0, b = 0, l = 0)\n      )\n  ),\n  rel_widths = c(1.4,1),\n  ncol = 2\n)\n\np_matching_indiv\n\nfilename &lt;- \"ternary-categ-matching-indiv\"\nggsave(\n  p_matching_indiv, file = str_c(path, filename, \".pdf\"),\n  height = 2*1.75, width = 3.75*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Counterfactuals for Categorical Features</span>"
    ]
  },
  {
    "objectID": "transp-categ.html#categorical-data-in-the-simplex",
    "href": "transp-categ.html#categorical-data-in-the-simplex",
    "title": "3  Counterfactuals for Categorical Features",
    "section": "3.2 Categorical Data in the Simplex",
    "text": "3.2 Categorical Data in the Simplex\nIn our example, for each observation, there exists a probability of belonging to each of the categories of the categorical variable, which is group-specific: \\(\\boldsymbol{p}_1 = (0.1, 0.5, 0.4)\\) and \\(\\boldsymbol{p}_0 = (0.5, 0.3, 0.2)\\).\nLet us generate again some observation in both groups, using the same vectors of probabilities as in Section 3.1:\n\nset.seed(1234)\nn &lt;- 100\nn0 &lt;- n1 &lt;- n\np0 &lt;- c(0.5, 0.3, 0.2)\np1 &lt;- c(0.1, 0.5, 0.4)\n# Sample category\nx0 &lt;- sample(c(\"A\", \"B\", \"C\"), size = n0, replace = TRUE, prob = p0)\nx1 &lt;- sample(c(\"A\", \"B\", \"C\"), size = n1, replace = TRUE, prob = p1)\ncat_levels &lt;- c(\"A\", \"B\", \"C\")\n\nNow, assume we were able to estimate the propensities of belonging to each category, using a classifier. Instead of really training a classifier here, we will simply draw the values from a Dirichlet distribution, using rdirichlet() from {MCMCpack}. We consider two different situations, with more or less concentration around the mean.\n\nlibrary(MCMCpack)\n\n\nFirst situation: lower concentrationSecond situation: higher concentration\n\n\n\nset.seed(12345)\nalpha_A &lt;- c(9, 3, 2)\nZ_A &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_A))\nalpha_B &lt;- c(3, 11, 4)\nZ_B &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_B))\nalpha_C &lt;- c(2, 3, 9)\nZ_C &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_C))\n# For each observation from group 0 and matched obs from group 1, we have\n# drawn a category (A, B, or C).\n# We add drawn propensities, depending on the category\nZ &lt;- Z_A\ncategory &lt;- c(x0, x1)\nZ[category == \"B\", ] &lt;- Z_B[category == \"B\", ]\nZ[category == \"C\", ] &lt;- Z_C[category == \"C\", ]\ntb_sample_z &lt;- as_tibble(Z)\nnames(tb_sample_z) &lt;- c(\"A\", \"B\", \"C\")\ntb_sample_z$group &lt;- factor(c(rep(0, n0), rep(1, n1)), levels = c(0, 1))\n\ntb_sample_z_1 &lt;- tb_sample_z\ntb_sample_z_1\n\n# A tibble: 200 × 4\n        A     B      C group\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1 0.668  0.243 0.0888 0    \n 2 0.112  0.618 0.270  0    \n 3 0.214  0.571 0.215  0    \n 4 0.286  0.566 0.148  0    \n 5 0.0988 0.204 0.697  0    \n 6 0.224  0.495 0.281  0    \n 7 0.373  0.542 0.0851 0    \n 8 0.735  0.195 0.0703 0    \n 9 0.257  0.517 0.227  0    \n10 0.104  0.758 0.138  0    \n# ℹ 190 more rows\n\n\n\n\n\nset.seed(1234)\nalpha_A &lt;- c(19, 3, 2)\nZ_A &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_A))\nalpha_B &lt;- c(3, 17, 2)\nZ_B &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_B))\nalpha_C &lt;- c(2, 3, 17)\nZ_C &lt;- as.data.frame(rdirichlet(n0 + n1, alpha_C))\n# For each observation from group 0 and matched obs from group 1, we have\n# drawn a category (A, B, or C).\n# We add drawn propensities, depending on the category\nZ &lt;- Z_A\ncategory &lt;- c(x0, x1)\nZ[category == \"B\", ] &lt;- Z_B[category == \"B\", ]\nZ[category == \"C\", ] &lt;- Z_C[category == \"C\", ]\ntb_sample_z &lt;- as_tibble(Z)\nnames(tb_sample_z) &lt;- c(\"A\", \"B\", \"C\")\ntb_sample_z$group &lt;- factor(c(rep(0, n0), rep(1, n1)), levels = c(0, 1))\n\ntb_sample_z_2 &lt;- tb_sample_z\ntb_sample_z_2\n\n# A tibble: 200 × 4\n        A     B      C group\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1 0.732  0.162 0.106  0    \n 2 0.0438 0.910 0.0462 0    \n 3 0.144  0.772 0.0841 0    \n 4 0.208  0.638 0.153  0    \n 5 0.0477 0.414 0.538  0    \n 6 0.0319 0.847 0.121  0    \n 7 0.873  0.115 0.0118 0    \n 8 0.789  0.107 0.104  0    \n 9 0.265  0.627 0.108  0    \n10 0.0946 0.803 0.102  0    \n# ℹ 190 more rows\n\n\n\n\n\nThe categorical variable in the simplex, using the simulated propensity scores, can be visualized on a ternary plot, as in Figure 3.3.\n\n\nCodes to create the Figure.\np &lt;- ggtern(\n  data = tb_sample_z_1 |&gt; mutate(type = \"(1)\") |&gt; \n    bind_rows(\n      tb_sample_z_2 |&gt; mutate(type = \"(2)\")\n    ), \n  mapping = aes(x = A, y = B, z = C)) +\n  geom_point(size = 1, alpha = 0.7, mapping = aes(color = group)) +\n  scale_colour_manual(name = \"group\",values = col_group) +\n  facet_wrap(~ type) +\n  theme_light(base_size = font_size, base_family = font_family) +\n  theme_ggtern_paper() +\n  theme(\n    legend.title = element_text(size = .8 * font_size),\n    legend.text = element_text(size = .8 * font_size),\n    tern.axis.vshift = .08,\n    tern.axis.arrow.sep = .16,\n  ) +\n  # theme_latex(TRUE)\n  theme_hidetitles()\np\n\n\n\n\n\nFigure 3.3: Using propensity scores, we have of points \\(\\boldsymbol{x}_{0,i}\\)’s and \\(\\boldsymbol{x}_{1,i}\\)’s in the simplex \\(\\mathcal{S}_3\\).\n\n\n\n\n\n\n\n\n\n\nCodes to export the figures in PDF.\nfilename &lt;- \"ternary-categ-drawn\"\nggsave(\n  p, file = str_c(path, filename, \".pdf\"),\n  height = 2*1.75, width = 3.75*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))\n\n\nLet us apply 1-to-1 matching exactly as in Section 3.1, setting numeric values to each category: A=1, B=2, C=3.\n\nx0_index &lt;- match(x0, cat_levels)\nx1_index &lt;- match(x1, cat_levels)\ncost_matrix &lt;- outer(x0_index, x1_index, function(i, j) abs(i - j))\n# 1-1 matching\nassignment &lt;- solve_LSAP(cost_matrix)\n# Store this in a tibble\ntb_coupling &lt;- tibble(\n  x0 = x0,\n  x1 = x1[assignment]\n) |&gt; \n  mutate(\n    cost = abs(match(x0, cat_levels) - match(x1, cat_levels))\n  )\n\nThe matching can be visualized on a ternary plot (Figure 3.4).\n\nidx &lt;- which(tb_coupling$cost != 0)\n\n# Ise the plot from previous figure as a baseline\np_matching &lt;- p\n\n# Draw a line joining the matched observations.\nfor (i in idx) {\n  lines_1 &lt;- f_line_simplex(\n    x = tb_sample_z_1[i, 1:3], \n    y = tb_sample_z_1[n + assignment[i], 1:3], \n    lgt = 101\n  )\n  lines_2 &lt;- f_line_simplex(\n    x = tb_sample_z_2[i, 1:3], \n    y = tb_sample_z_2[n + assignment[i], 1:3], \n    lgt = 101\n  )\n  lines_both &lt;- as_tibble(lines_1) |&gt; mutate(type = \"(1)\") |&gt; \n    bind_rows(\n      as_tibble(lines_2) |&gt; mutate(type = \"(2)\")\n    )\n  \n  p_matching &lt;- p_matching + \n    geom_line(\n      data = lines_both, \n      mapping = aes(x = A, y = B, z = C), \n      color = col_group[2], linewidth = .2,, alpha = .5,\n      arrow = arrow(length = unit(0.20, \"cm\"))\n    )\n}\n\np_matching\n\n\n\n\nFigure 3.4: Optimal matching of \\(\\boldsymbol{p}_{0,i}\\)’s and \\(\\boldsymbol{p}_{1,i}\\)’s in the simplex \\(\\mathcal{S}_3\\).\n\n\n\n\n\n\n\n\n\n\nCodes to export the figures in PDF.\nfilename &lt;- \"ternary-categ-ot\"\nggsave(\n  p_matching, file = str_c(path, filename, \".pdf\"),\n  height = 2*1.75, width = 3.75*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Counterfactuals for Categorical Features</span>"
    ]
  },
  {
    "objectID": "barycentric.html",
    "href": "barycentric.html",
    "title": "4  Barycentric Centroid of Balance",
    "section": "",
    "text": "4.1 Varying \\(\\alpha\\) and \\(\\boldsymbol{p}\\).\n\\[\n\\definecolor{wongBlack}{RGB}{0,0,0}\n\\definecolor{wongGold}{RGB}{230, 159, 0}\n\\definecolor{wongLightBlue}{RGB}{86, 180, 233}\n\\definecolor{wongGreen}{RGB}{0, 158, 115}\n\\definecolor{wongYellow}{RGB}{240, 228, 66}\n\\definecolor{wongBlue}{RGB}{0, 114, 178}\n\\definecolor{wongOrange}{RGB}{213, 94, 0}\n\\definecolor{wongPurple}{RGB}{204, 121, 167}\n\\definecolor{colA}{RGB}{255, 221, 85}\n\\definecolor{colB}{RGB}{148, 78, 223}\n\\definecolor{colC}{RGB}{63, 179, 178}\n\\definecolor{colGpeZero}{RGB}{127, 23, 14}\n\\definecolor{colGpeUn}{RGB}{27, 149, 224}\n\\]\nLet us consider, as in Chapter 3, a categorical variable \\(x \\in \\{{\\color{colA}A}, {\\color{colB}B}, {\\color{colC}C}\\}\\), with group-specific distributions. We draw random samples from a Dirichlet distribution. Let us start with a concentration parameter \\(\\alpha=(2, 5, 3)\\). Each sample lies in the 3-dimensional simplex \\(\\mathcal{S}_3\\), and the resulting distribution has density \\(f\\) with respect to Lebesgue measure on \\(\\mathcal{S}_3\\).\nWe want to find a partition \\(R_A,R_B,R_C\\) of \\(\\mathcal{S}_3\\) such that \\[\n\\int_{R_i}f(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}=p_i,~i\\in\\{A,B,C\\}.\n\\] We can use (classical) optimal transport theory to find partitions of the simplex that map probability mass from the Dirichlet to the discrete distribution \\(\\boldsymbol{p}=(p_A,p_B,p_C)\\), in each vertex of the simplex, \\[\n\\int_{T^{-1}(\\boldsymbol{u}_i)}f(\\boldsymbol{x})\\mathrm{d}\\boldsymbol{x}=p_i,~i\\in\\{A,B,C\\},\n\\] where \\(\\{\\boldsymbol{u}_A,\\boldsymbol{u}_B,\\boldsymbol{u}_C\\}\\) are unit vectors, vertices of the \\(\\mathcal{S}_3\\) (i.e., \\((1,0,0)\\), \\((0,1,0)\\) and \\((0,0,1)\\)).\nLet us consider for now that \\(\\boldsymbol{p}=(1/2,1/3,1/6))\\).\nEach region \\(R_i\\) corresponds to the set of points \\(\\boldsymbol{x}\\in\\mathcal{S}_3\\) such that \\(\\boldsymbol{u}_i\\) minimizes \\(\\|\\boldsymbol{x}-\\boldsymbol{u}_i\\|^2-\\phi_i\\) where \\(\\phi_i\\in\\mathbb{R}\\) is a potential offset (weight, determined via dual optimization). This structure defines a power diagram, also known as a Laguerre–Voronoi diagram (or additively weighted Voronoi diagram). These subsets form a weighted Voronoi tessellation (in barycentric space), as shown in (simplex-baryc-centr-bal-example?).\nWe can, in addition, identify the intersection, i.e., the point where the minimum of the class-wise kernel density estimates is maximized. We do it by considering a grid over which we estimate the density. We create a function, generate_simplex_grid(), to generate a grid on \\(\\mathcal{S}_3\\).\nWe create a function, get_category_density_2D(), to estimate the kernel density estumation for a given category.\nLet us create a triangular grid over (A,B,C) constrained to S_3:\nWe evaluate the KDE for data from the source distribution:\nThen, we can find the point where min(densities) is maximal.\nLet us now make \\(\\alpha\\) and \\(\\boldsymbol{p}\\) vary. We will consider \\(\\boldsymbol{\\alpha} = (1,1,1)\\) (uniform distribution), and \\(\\boldsymbol{\\alpha} = (2, 5, 3)\\); and \\(\\boldsymbol{p} = (1/3,1/3,1/3)\\), and \\(\\boldsymbol{p} = (1/2,1/3,1/6)\\).\nFor convenience, let us wrap the previous code in a function, get_data_assignment().\nCode for the get_data_assignment function\n#' @param n Number of observations to sample from the Dirichlet Distribution.\n#' @param Vector of shape parameters, or matrix of shape parameters \n#'  corresponding to the number of draw. Default to \\eqn{(1,1,1)}.\n#' @param p Vector of target probabilities. Default to \\eqn{(1/3, 1/3, 1/3)}.\n#' \nget_data_assignment &lt;- function(n,\n                                alpha = c(1, 1, 1),\n                                p = c(1, 1, 1) / 3,\n                                intersection_point = TRUE) {\n  \n  # Draw n samples\n  samples &lt;- rdirichlet(n, alpha = alpha)\n  \n  # Unit vectors of S_3\n  vertices &lt;- matrix(c(\n    1, 0, 0,  # A\n    0, 1, 0,  # B\n    0, 0, 1   # C\n  ), byrow = TRUE, ncol = 3)\n  \n  # source weights\n  mass_source &lt;- rep(1 / n, n)\n  # target weights\n  mass_target &lt;- p\n  \n  # Cost matrix (squared Euclidean distance)\n  cost_matrix &lt;- as.matrix(dist(rbind(samples, vertices))^2)\n  cost_matrix &lt;- cost_matrix[1:n, (n + 1):(n + 3)]\n  \n  # We assign eah observation to one vertex\n  # by minimizing the global transport cost, while matching marginals\n  \n  # Solve the optimal transport plan\n  ot_plan &lt;- transport::transport(\n    a = mass_source, b = mass_target, costm = cost_matrix, \n    method = \"shortsimplex\"\n  )\n  \n  # Assign each sample to a category based on OT plan\n  assignment &lt;- rep(NA, n)\n  # mass each source sends to each target\n  mass_matrix &lt;- matrix(0, nrow = n, ncol = 3)\n  \n  for (j in 1:nrow(ot_plan)) {\n    from &lt;- ot_plan$from[j]\n    to &lt;- ot_plan$to[j]\n    mass &lt;- ot_plan$mass[j]\n    mass_matrix[from, to] &lt;- mass_matrix[from, to] + mass\n  }\n  \n  # Assign each source point to the target it contributes the most mass to\n  assignment &lt;- max.col(mass_matrix, ties.method = \"first\")\n  \n  colnames(samples) &lt;- c(\"A\", \"B\", \"C\")\n  samples &lt;- \n    as_tibble(samples) |&gt; \n    mutate(category = colnames(samples)[assignment])\n  \n  #\n  # Intersection point\n  #\n  if (intersection_point == TRUE) {\n    # Create a triangular grid over (A,B,C) constrained to S_3\n    grid_points &lt;- generate_simplex_grid(resolution = 100)\n    \n    # Evaluation of KDE for data from the source distribution\n    dens_A &lt;- get_category_density_2D(\n      samples = samples, grid_points = grid_points, category_label = \"A\"\n    )\n    dens_B &lt;- get_category_density_2D(\n      samples = samples, grid_points = grid_points, category_label = \"B\"\n    )\n    dens_C &lt;- get_category_density_2D(\n      samples = samples, grid_points = grid_points, category_label = \"C\"\n    )\n    # Find point where min(densities) is maximal\n    min_dens &lt;- pmin(dens_A, dens_B, dens_C)\n    max_idx &lt;- which.max(min_dens)\n    \n    intersection_point &lt;- grid_points[max_idx, ]\n    \n    tb_intersection &lt;- as_tibble(intersection_point)\n  } else {\n    tb_intersection &lt;- NULL\n  }\n  \n  list(\n    samples = samples,\n    tb_intersection = tb_intersection\n  )\n}\nUsing get_data_assignment(), we draw samples according to \\(\\boldsymbol{\\alpha}\\) and then we find partitions of the simplex that map probability mass from the Dirichlet to the discrete distribution \\(\\boldsymbol{p}\\), in each vertex of the simplex.\nsamples_unif_unif &lt;- get_data_assignment(\n  n = n, \n  alpha = c(1, 1, 1), \n  p = c(1, 1, 1) / 3\n)\n\nsamples_unif_p &lt;- get_data_assignment(\n  n = n, \n  alpha = c(1, 1, 1), \n  p = c(3, 2, 1) / 6\n)\n\nsamples_dirichlet_unif &lt;- get_data_assignment(\n  n = n, \n  alpha = c(2, 5, 3), \n  p = c(1, 1, 1) / 3\n)\n\nsamples_dirichlet_p &lt;- get_data_assignment(\n  n = n, \n  alpha = c(2, 5, 3), \n  p = c(3, 2, 1) / 6\n)\nThe results can be visualized in simplex-baryc-centr-bal-example-full, when \\(\\mathcal{D}(\\boldsymbol{\\alpha})\\) with \\(\\boldsymbol{\\alpha}=(1, 1, 1)\\) (left) and \\(\\boldsymbol{\\alpha}=(2, 5, 3)\\) (right), when \\(\\boldsymbol{p}=(1,1,1)/3\\) (top) and \\(\\boldsymbol{p}=(3,2,1)/6\\) (bottom).\nCodes to create the Figure.\np &lt;- ggtern(\n  data = samples_unif_unif$samples |&gt; \n    mutate(distrib = \"Uniform\", p = \"(1,1,1)/3\") |&gt; \n    bind_rows(\n      samples_unif_p$samples |&gt; \n        mutate(distrib = \"Uniform\", p = \"(3,2,1)/6\")\n    ) |&gt; \n    bind_rows(\n      samples_dirichlet_unif$samples |&gt; \n        mutate(distrib = \"2_5_3\", p = \"(1,1,1)/3\")\n    ) |&gt; \n    bind_rows(\n      samples_dirichlet_p$samples |&gt; \n        mutate(distrib = \"2_5_3\", p = \"(3,2,1)/6\")\n    ) |&gt;\n    mutate(\n      distrib = factor(\n        distrib,\n        labels = c(\n          \"Uniform\" = parse(text = latex2exp::TeX(\"D(1,1,1)\")),\n          \"2_5_3\" = parse(text = latex2exp::TeX(\"$D(2,5,3)$\"))\n        )\n      ),\n      p = factor(\n        p,\n        labels = c(\n          \"(1,1,1)/3\" = parse(text = latex2exp::TeX(\"$p=(1,1,1)/3$\")),\n          \"(3,2,1)/6\" = parse(text = latex2exp::TeX(\"$p=(3,2,1)/6$\"))\n        )\n      )\n    ),\n  mapping = aes(x = A, y = B, z = C)\n) +\n  geom_point(alpha = .8, size = .5, mapping = aes(color = category)) +\n  geom_point(\n    data = samples_unif_unif$tb_intersection |&gt; \n      mutate(distrib = \"Uniform\", p = \"(1,1,1)/3\") |&gt; \n      bind_rows(\n        samples_unif_p$tb_intersection |&gt; \n          mutate(distrib = \"Uniform\", p = \"(3,2,1)/6\")\n      ) |&gt; \n      bind_rows(\n        samples_dirichlet_unif$tb_intersection |&gt; \n          mutate(distrib = \"2_5_3\", p = \"(1,1,1)/3\")\n      ) |&gt; \n      bind_rows(\n        samples_dirichlet_p$tb_intersection |&gt; \n          mutate(distrib = \"2_5_3\", p = \"(3,2,1)/6\")\n      ) |&gt;\n      mutate(\n        distrib = factor(\n          distrib,\n          labels = c(\n            \"Uniform\" = parse(text = latex2exp::TeX(\"D(1,1,1)\")),\n            \"2_5_3\" = parse(text = latex2exp::TeX(\"$D(2,5,3)$\"))\n          )\n        ),\n        p = factor(\n          p,\n          labels = c(\n            \"(1,1,1)/3\" = parse(text = latex2exp::TeX(\"$p=(1,1,1)/3$\")),\n            \"(3,2,1)/6\" = parse(text = latex2exp::TeX(\"$p=(3,2,1)/6$\"))\n          )\n        )\n      )\n  ) +\n  scale_colour_manual(values = col_categ) +\n  facet_grid(p ~ distrib, labeller = label_parsed, switch = \"y\") +\n  theme_light(base_size = font_size, base_family = font_family) +\n  theme(\n    strip.background = element_rect(colour = \"black\", fill = NA),\n    strip.text.x = element_text(colour = \"black\"),\n    strip.text.y = element_text(colour = \"black\"),\n    text = element_text(family = font_family, size = unit(font_size, \"pt\")),\n    axis.title = element_text(size = rel(.8)),\n    tern.axis.arrow.show = TRUE,\n    tern.axis.arrow.sep = .16,\n    tern.axis.vshift = .09,\n    legend.position = \"bottom\",\n    legend.title = element_text(size = .8 * font_size),\n    legend.text = element_text(size = .8 * font_size),\n    panel.border = element_rect(colour = NA)\n  ) +\n  theme_hidetitles() +\n  guides(colour = guide_legend(override.aes = list(size = 2)))\n\np\n\n\n\n\n\nFigure 4.2: Barycentric centroid of balance, when \\(\\mathcal{D}(\\boldsymbol{\\alpha})\\) with \\(\\boldsymbol{\\alpha}=(1, 1, 1)\\) (left) and \\(\\boldsymbol{\\alpha}=(2, 5, 3)\\) (right), when \\(\\boldsymbol{p}=(1,1,1)/3\\) (top) and \\(\\boldsymbol{p}=(3,2,1)/6\\) (bottom). Black dot: intersection (point where the minimum of the class-wise kernel density estimates is maximized).\nCodes to export the figure in PDF.\nfilename &lt;- \"baryc-centr-bal\"\nggsave(\n  p, file = str_c(path, filename, \".pdf\"),\n  height = 3.3*1.75, width = 3.25*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Barycentric Centroid of Balance</span>"
    ]
  },
  {
    "objectID": "transport-categ-toy.html",
    "href": "transport-categ-toy.html",
    "title": "5  Transporting a Categorical Variable",
    "section": "",
    "text": "5.1 Setup\n\\[\n\\definecolor{wongBlack}{RGB}{0,0,0}\n\\definecolor{wongGold}{RGB}{230, 159, 0}\n\\definecolor{wongLightBlue}{RGB}{86, 180, 233}\n\\definecolor{wongGreen}{RGB}{0, 158, 115}\n\\definecolor{wongYellow}{RGB}{240, 228, 66}\n\\definecolor{wongBlue}{RGB}{0, 114, 178}\n\\definecolor{wongOrange}{RGB}{213, 94, 0}\n\\definecolor{wongPurple}{RGB}{204, 121, 167}\n\\definecolor{colA}{RGB}{255, 221, 85}\n\\definecolor{colB}{RGB}{148, 78, 223}\n\\definecolor{colC}{RGB}{63, 179, 178}\n\\definecolor{colGpeZero}{RGB}{127, 23, 14}\n\\definecolor{colGpeUn}{RGB}{27, 149, 224}\n\\]\nAs in the previous pages, assume two groups: 0 and 1. In the first group, there are \\(n_0=6\\) individuals indexed 1, 2, 3, 4, 5, 6; and in group 1, there are \\(n_1=6\\) individuals indexed 7, 8, 9, 10, 11, 12. Let \\(Y\\) denote a response variable that takes values in \\(\\mathbb{R}\\), and let \\(X\\) be a categorical variable taking values \\(\\{A,B,C\\}\\).\nLet us assume that we obtained the estimated probabilities of being in each class using a multinomial regression model. This allows to convert categorical observations \\(\\{x_{1,1},\\cdots,x_{1,n_1}\\}\\) and \\(\\{x_{0,1},\\cdots,x_{0,n_0}\\}\\) into estimated probabilities, \\(\\{\\boldsymbol{p}_{1,1},\\cdots,\\boldsymbol{p}_{1,n_1}\\}\\) and \\(\\{\\boldsymbol{p}_{0,1},\\cdots,\\boldsymbol{p}_{0,n_0}\\}\\).\nLet us create a toy example:\ngroup_0 &lt;- tribble(\n  ~i, ~x, ~p_A, ~p_B, ~p_C, ~y,\n  1, \"A\", .8,   .1,   .1,   1,\n  2, \"A\", .7,   .2,   .1,   2,\n  3, \"A\", .6,   .1,   .3,   3,\n  4, \"B\", .2,   .7,   .1,   4,\n  5, \"B\", .3,   .6,   .1,   5,\n  6, \"C\", .1,   .2,   .7,   6\n)\n\ngroup_1 &lt;- tribble(\n  ~i, ~x, ~p_A, ~p_B, ~p_C, ~y,\n  7,  \"A\", .7,  .2,   .1,   3,\n  8,  \"B\", .1,  .7,   .2,   4,\n  9,  \"B\", .6,  .2,   .2,   5,\n  10, \"B\", .2,  .5,   .3,   6,\n  11, \"C\", .3,  .1,   .6,   7,\n  12, \"C\", .1,  .3,   .6,   8\n)",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transporting a Categorical Variable</span>"
    ]
  },
  {
    "objectID": "gaussian-example.html",
    "href": "gaussian-example.html",
    "title": "2  Gaussian Example",
    "section": "",
    "text": "2.1 Data Generating Process\n\\[\n\\definecolor{wongBlack}{RGB}{0,0,0}\n\\definecolor{wongGold}{RGB}{230, 159, 0}\n\\definecolor{wongLightBlue}{RGB}{86, 180, 233}\n\\definecolor{wongGreen}{RGB}{0, 158, 115}\n\\definecolor{wongYellow}{RGB}{240, 228, 66}\n\\definecolor{wongBlue}{RGB}{0, 114, 178}\n\\definecolor{wongOrange}{RGB}{213, 94, 0}\n\\definecolor{wongPurple}{RGB}{204, 121, 167}\n\\definecolor{colA}{RGB}{255, 221, 85}\n\\definecolor{colB}{RGB}{148, 78, 223}\n\\definecolor{colC}{RGB}{63, 179, 178}\n\\definecolor{colGpe1}{RGB}{127, 23, 14}\n\\definecolor{colGpe0}{RGB}{27, 149, 224}\n\\]\nWe want to simulate potential outcomes in a binary treatment setting, with covariate shift between treatment groups.\nLet \\(n=500\\) denote the number of individuals (or unit), and let \\(\\boldsymbol{X}=(X_1,X_2)\\) be drawn from bivariate normal distrubtions whose mean vectors and covariance matrices depend on the treatment assignment \\(A\\in\\{0,1\\}\\).\nFor untreated individuals (\\(A=\\color{colGpe0}0\\)) the covariates \\(\\boldsymbol{X}^{(0)} = (X_1^{(0)}, X_2^{(0)})\\) are sampled from a \\(\\mathcal{N}(\\mu_0, \\Sigma_0)\\), where \\(\\mu_0 = -1\\), \\(\\Sigma_0 = \\begin{pmatrix} 1 & r_0 \\\\ r_0 & 1 \\end{pmatrix}\\) with \\(r_0 = 0.7\\).\nFor treated individuals (\\(A=\\color{colGpe1}1\\)), covariates \\(\\boldsymbol{X}^{(1)} = (X_1^{(1)}, X_2^{(1)})\\) follow a \\(\\mathcal{N}(\\mu_1, \\Sigma_1)\\), where \\(\\mu_1 = +1\\), \\(\\Sigma_1 = \\begin{pmatrix} 1 & r_1 \\\\ r_1 & 1 \\end{pmatrix}\\) with \\(r_1 = -0.5\\).\nThe treatment assignment \\(A\\) is randomized with probability \\(p_1 = 0.5\\).\nThe potential outcomes are linear functions of the covariates: \\[\n\\begin{aligned}\nY(0) &= a_1 X_1 + a_2 X_2 + \\varepsilon,\\\\\nY(1) &= a_1 X_1 + a_2 X_2 + a_0 + \\varepsilon .\n\\end{aligned}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal{N}(0, 1)\\) and \\(a_0 = 3\\), \\(a_1 = 2\\), \\(a_2 = -1.5\\).\nThe observed outcome is \\[Y = A \\cdot Y(1) + (1 - A) \\cdot Y(0).\\]\nWe will focus on the average treatment effect of the treated: \\[\n\\begin{aligned}\n\\text{ATT}\n&= \\mathbb{E}\\bigl[Y(1)-Y(0)\\mid A=1\\bigr] \\\\\n&= a_0 \\\\\n&= 3.\n\\end{aligned}\n\\]\nset.seed(12345)\n# Parameters\nn &lt;- 500\nmu0 &lt;- -1\nmu1 &lt;- +1\nr0 &lt;- +.7\nr1 &lt;- -.5\na &lt;- 1\na0 &lt;-  3\na1 &lt;-  2\na2 &lt;-  -1.5\np1 &lt;- .5\nMu0 &lt;- rep(mu0, 2)\nMu1 &lt;- rep(mu1, 2)\nSig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\nSig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n\n# Draw covariates\nX0 &lt;- rmnorm(n, mean = a * Mu0, varcov = Sig0)\nX1 &lt;- rmnorm(n, mean = a * Mu1, varcov = Sig1)\n# Random noise\nE &lt;- rnorm(n)\n# Binary treatment\nA &lt;- sample(0:1, size = n, replace = TRUE, prob = c(1 - p1, p1))\n\nX &lt;- X0\nX[A==1, ] = X1[A==1, ]\n\ndf &lt;- tibble(\n  X1 = X[, 1],\n  X2 = X[, 2],\n  A = A,\n  Y0 = a1 * X1 + a2 * X2 + E,\n  Y1 = a1 * X1 + a2 * X2 + a0 + E,\n  Y = A * Y1 + (1-A) * Y0\n)\nWe define a function to wrap this DGP.\nThe gen_data() function.\n#' @param n Number of units.\n#' @param mu0 Mean of the two covariates in group 0.\n#' @param mu1 Mean of the two covariates in group 1.\n#' @param r0 Covariance of the two covariates in group 0.\n#' @param r1 Covariance of the two covariates in group 1.\n#' @parma a Shift parameter for the mean in both groups\n#'  (default to 1: no shift). Larger values decreases overlapping.\ngen_data &lt;- function(n = 500,\n                     mu0 = -1,\n                     mu1 = +1,\n                     r0 = +.7,\n                     r1 = -.5,\n                     a = 1,\n                     seed = NULL) {\n  \n  if (!is.null(seed)) set.seed(seed)\n  \n  a0 &lt;-  3\n  a1 &lt;-  2\n  a2 &lt;-  -1.5\n  p1 &lt;- .5\n  Mu0 &lt;- rep(mu0, 2)\n  Mu1 &lt;- rep(mu1, 2)\n  Sig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\n  Sig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n  # Draw covariates\n  X0 &lt;- rmnorm(n, mean = a * Mu0, varcov = Sig0)\n  X1 &lt;- rmnorm(n, mean = a * Mu1, varcov = Sig1)\n  # Random noise\n  E &lt;- rnorm(n)\n  # Binary treatment\n  A &lt;- sample(0:1, size = n, replace = TRUE, prob = c(1 - p1, p1))\n  X &lt;- X0\n  X[A==1, ] = X1[A==1, ]\n  df &lt;- tibble(\n    X1 = X[, 1],\n    X2 = X[, 2],\n    A = A,\n    Y0 = a1 * X1 + a2 * X2 + E,\n    Y1 = a1 * X1 + a2 * X2 + a0 + E,\n    Y = A * Y1 + (1-A) * Y0\n  )\n  \n  df\n}",
    "crumbs": [
      "I. Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gaussian Example</span>"
    ]
  },
  {
    "objectID": "gaussian-example.html#data-generating-process",
    "href": "gaussian-example.html#data-generating-process",
    "title": "2  Gaussian Example",
    "section": "",
    "text": "Monte-Carlo Simulations to Compute the ATT\n\n\n\n\n\nWe generate a dataset using the DGP previously explained and compute the true ATT. We replicate this 5,000 times. For convenience, we wrap the previous codes in a function and compute the true ATT within that function.\n\nsimul_check &lt;- function(seed) {\n  \n  set.seed(seed)\n  # Parameters\n  n &lt;- 500\n  mu0 &lt;- -1\n  mu1 &lt;- +1\n  r0 &lt;- +.7\n  r1 &lt;- -.5\n  a &lt;- 1\n  a0 &lt;-  3\n  a1 &lt;-  2\n  a2 &lt;-  -1.5\n  p1 &lt;- .5\n  Mu0 &lt;- rep(mu0, 2)\n  Mu1 &lt;- rep(mu1, 2)\n  Sig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\n  Sig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n  \n  # Draw covariates\n  X0 &lt;- rmnorm(n, mean = a * Mu0, varcov = Sig0)\n  X1 &lt;- rmnorm(n, mean = a * Mu1, varcov = Sig1)\n  # Random noise\n  E &lt;- rnorm(n)\n  # Binary treatment\n  A &lt;- sample(0:1, size = n, replace = TRUE, prob = c(1 - p1, p1))\n  \n  df &lt;- tibble(\n    X1_0 = X0[, 1],\n    X2_0 = X0[, 2],\n    X1_1 = X1[, 1],\n    X2_1 = X1[, 2],\n    X1 = A * X1_1 + (1-A) * X1_0,\n    X2 = A * X2_1 + (1-A) * X2_0,\n    A = A,\n    Y0 = a1 * X1_0 + a2 * X2_0 + E,\n    Y1 = a1 * X1_1 + a2 * X2_1 + a0 + E,\n    Y = A * Y1 + (1-A) * Y0\n  )\n  \n  ## Observed outcome\n  Y &lt;- df$Y\n  ## Potential outcomes\n  Y0 &lt;- df$Y0\n  Y1 &lt;- df$Y1\n  # ATT\n  ATT_true &lt;- mean((Y1 - Y0)[A == 1])\n  \n  ATT_true\n}\n\nLet us call that function 5,000 times.\n\n# This chunk takes about 4 seconds to run.\n# We do not evaluate during compilation. Instead, we load previously\n# obtained results.\nout &lt;- pbapply::pblapply(1:5000, simul_check) |&gt; \n  list_c()\nif (!dir.exists(\"../output/\")) dir.create(\"../output/\")\nsave(out, file = \"../output/gaussian-example-mc-out.rda\")\n\nThe mean and standard deviation of the ATE using the DPG described above:\n\nload(\"../output/gaussian-example-mc-out.rda\")\nc(ATT_true_est = mean(out), ATT_true_est = sd(out))\n\nATT_true_est ATT_true_est \n   3.9972712    0.2122866 \n\n\nThe distribution of the true ATE computed over 5,000 replications is shown in Figure 2.1.\n\n\nCodes to create the Figure.\nggplot(data = tibble(ATT = out)) +\n  geom_histogram(\n    mapping = aes(x = ATT), colour = \"black\", fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(out), col = \"blue\", linetype = \"dashed\", linewidth = 2\n  ) +\n  labs(y = \"Freq.\") +\n  theme_paper()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 2.1: Distribution of the estimate of the ATE over 5,000 Monte-Carlo simulations.",
    "crumbs": [
      "I. Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gaussian Example</span>"
    ]
  },
  {
    "objectID": "gaussian-example.html#a-closer-look-at-overlapping",
    "href": "gaussian-example.html#a-closer-look-at-overlapping",
    "title": "2  Gaussian Example",
    "section": "2.2 A Closer Look at Overlapping",
    "text": "2.2 A Closer Look at Overlapping\nWhen the estimation of the ATT is done using the AIPW estimator, the procedure requires a propensity-score model: \\[e(X) = \\mathbb{P}(A = 1 \\mid X),\\] and an outcome regression for the untreated potential outcome: \\[\\mu_0(\\boldsymbol{X}) = \\mathbb{E}[Y \\mid \\boldsymbol{X}, A = 0].\\]\nThe ATT is then estimated as follows: \\[\n\\widehat{\\text{ATT}}_{\\text{AIPW}} = \\frac{1}{n_1} \\sum_{i: A_i = 1} \\left[ Y_i - \\hat{\\mu}_0(X_i) \\right] + \\frac{1}{n_1} \\sum_{i: A_i = 0} \\frac{\\hat{e}(X_i)}{1 - \\hat{e}(X_i)} \\left[ Y_i - \\hat{\\mu}_0(X_i) \\right]\n\\tag{2.1}\\]\nThe first term imputes the counterfactual outcome for treated units, whereas the second corrects any residual bias by re-weighting control residuals.\nConsistency holds if either the propensity-score model or the outcome model is correctly specified (known as the estimator’s double robustness).\nFor every covariate pattern represented among treated units, the identification assumption is required: \\[0 &lt; \\mathbb{P}(A = 1 \\mid X) &lt; 1\\] This assumption ensures an adequate overlap and prevents infinite weights.\nIn finite samples, extreme estimated propensities (\\(\\hat{e}(\\boldsymbol{X})\\approx 0\\) or \\(\\hat{e}(\\boldsymbol{X})\\approx 1\\)) lead to inflated variance. A common practice is then to introduce a trimming rule that discards observations with \\(\\hat{e}(\\boldsymbol{X})\\) outside a pre-specified interval. Common values are \\([0.05;0.95]\\) or \\([0.01;0.99]\\).\nHere, we want to have a closer look at what happens when trimming.\nLet us consider two examples:\n\nLarge overlap: \\(\\mu_0=-.5\\), \\(\\mu_1=.5\\), the rest of the parameters are the same as those presented above.\nLimited overlap: \\(\\mu_0=-1.5\\), \\(\\mu_1=1.5\\), the rest of the parameters are the same as those presented above.\n\n\n\nCodes to create the Figure.\n# First example\nmu0 &lt;- -.5\nmu1 &lt;- .5\nr0 &lt;- +.7\nr1 &lt;- -.5\na &lt;- 1\np1 &lt;- .5\nMu0 &lt;- rep(mu0, 2)\nMu1 &lt;- rep(mu1, 2)\nSig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\nSig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n\nu &lt;- seq(-10, 10, length = 161)\ndxy &lt;- expand_grid(X1 = u, X2 = u)\n\nd0 &lt;- dmnorm(as.matrix(dxy), mean = a * Mu0, varcov = Sig0)\nd1 &lt;- dmnorm(as.matrix(dxy), mean = a * Mu1, varcov = Sig1)\n\nz0 &lt;- matrix(d0, nrow = length(u))\nz1 &lt;- matrix(d1, nrow = length(u))\n\npar(mar = c(2.1, 2.1, 0.1, 0.1), mfrow = c(1,2))\nx_lim &lt;- c(-3, 3)\ny_lim &lt;- c(-4, 4)\n\nplot(NA,NA, \n     xlim = x_lim, ylim = y_lim, \n     xlab = \"\", ylab = \"\", pch = NA,\n     family = font_family\n)\ntitle(xlab = \"X1\", ylab=\"X2\", line=2, cex.lab=1.2, family = font_family)\n\n\n\n# posterior probability of class 1 (using Bayes' theorem)\nposterior &lt;- (p1 * d1) / ((1-p1) * d0 + p1 * d1)\n# reshape for plotting\npost_matrix &lt;- matrix(posterior, nrow = length(u), byrow = FALSE)\n\nlvl &lt;- c(.01, .99)\n\n# contour(\n# u, u, post_matrix, levels = lvl, drawlabels = TRUE,\n# col = \"blue\", lty = 3, lwd = 2,\n# xlab = \"X1\", ylab = \"X2\", main = \"Region where 0.01 ≤ P(A|X) ≤ 0.99\"\n# )\n\n# extract contour lines\ncontours &lt;- contourLines(u, u, post_matrix, levels = lvl)\nparam_dens &lt;- 35\ncl &lt;- contours\nfor (i in 1:length(cl)) {\n  if (cl[[i]]$x[2] &lt; cl[[i]]$x[1]) {\n    cl[[i]]$x &lt;- rev(cl[[i]]$x)\n    cl[[i]]$y &lt;- rev(cl[[i]]$y)\n  }\n}\n\n# Bottom-left\npolygon(\n  c(cl[[1]]$x, 10, -10, -10),\n  c(cl[[1]]$y, -10, -10, 10),\n  col=scales::alpha(colGpe0, .9), density = param_dens,\n  border = NA\n)\n# Upper-right\npolygon(\n  c(cl[[2]]$x, 10, 10, -10),\n  c(cl[[2]]$y, -10, 10, 10),\n  col=scales::alpha(colGpe0, .9), density = param_dens,\n  border = NA\n)\n# Upper-left\npolygon(\n  c(cl[[3]]$x, 10, -10, -10),\n  c(cl[[3]]$y, 10, 10, -10),\n  col=scales::alpha(colGpe1, .9), density = param_dens,\n  border = NA\n)\n# Bottom-right\npolygon(\n  c(cl[[4]]$x, 10, 10, -10),\n  c(cl[[4]]$y, 10, -10, -10),\n  col=scales::alpha(colGpe1, .9), density = param_dens,\n  border = NA\n)\n\ncontour_lwr &lt;- cl[sapply(cl, function(x) x$level == lvl[1])]\ncontour_upr &lt;- contours[sapply(cl, function(x) x$level == lvl[2])]\nfor (c in contour_lwr) lines(c$x, c$y, col = \"gray30\", lty = 1)\nfor (c in contour_upr) lines(c$x, c$y, col = \"gray30\", lty = 1)\n\n# Add contour lines\ncontour(u, u, z0, add = TRUE, lwd = 1, col = colGpe0, family = font_family, labcex = 1)\ncontour(u, u, z1, add = TRUE, lwd = 1, col = colGpe1, family = font_family, labcex = 1)\n\n# Second example\nmu0 &lt;- -1.5\nmu1 &lt;- 1.5\nr0 &lt;- +.7\nr1 &lt;- -.5\na &lt;- 1\np1 &lt;- .5\nMu0 &lt;- rep(mu0, 2)\nMu1 &lt;- rep(mu1, 2)\nSig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\nSig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n\nu &lt;- seq(-10, 10, length = 161)\ndxy &lt;- expand_grid(X1 = u, X2 = u)\n\nd0 &lt;- dmnorm(as.matrix(dxy), mean = a * Mu0, varcov = Sig0)\nd1 &lt;- dmnorm(as.matrix(dxy), mean = a * Mu1, varcov = Sig1)\n\nz0 &lt;- matrix(d0, nrow = length(u))\nz1 &lt;- matrix(d1, nrow = length(u))\n\n# x_lim &lt;- y_lim &lt;- c(-3.5, 3.5)\nx_lim &lt;- c(-3, 3)\ny_lim &lt;- c(-4, 4)\n\nplot(NA,NA, \n     xlim = x_lim, ylim = y_lim, \n     xlab = \"\", ylab = \"\", pch = NA,\n     family = font_family\n)\ntitle(xlab = \"X1\", ylab=\"X2\", line=2, cex.lab=1.2, family = font_family)\n# posterior probability of class 1 (using Bayes' theorem)\nposterior &lt;- (p1 * d1) / ((1-p1) * d0 + p1 * d1)\n# reshape for plotting\npost_matrix &lt;- matrix(posterior, nrow = length(u), byrow = FALSE)\n\nlvl &lt;- c(.01, .99)\n\n# contour(\n# u, u, post_matrix, levels = lvl, drawlabels = TRUE,\n# col = \"blue\", lty = 3, lwd = 2,\n# xlab = \"X1\", ylab = \"X2\", main = \"Region where 0.01 ≤ P(A|X) ≤ 0.99\"\n# )\n\n# extract contour lines\ncontours &lt;- contourLines(u, u, post_matrix, levels = lvl)\n\ncl &lt;- contours\nfor (i in 1:length(cl)) {\n  if (cl[[i]]$x[2] &lt; cl[[i]]$x[1]) {\n    cl[[i]]$x &lt;- rev(cl[[i]]$x)\n    cl[[i]]$y &lt;- rev(cl[[i]]$y)\n  }\n}\n\n# Bottom-left\npolygon(\n  c(cl[[1]]$x, 10, -10, -10),\n  c(cl[[1]]$y, -10, -10, 10),\n  col=scales::alpha(colGpe0, .9), density = param_dens,\n  border = NA\n)\n# Upper-right\npolygon(\n  c(cl[[2]]$x, 10, 10, -10),\n  c(cl[[2]]$y, -10, 10, 10),\n  col=scales::alpha(colGpe0, .9), density = param_dens,\n  border = NA\n)\n# Upper part (wrong but not for the cropped image)\npolygon(\n  c(cl[[3]]$x, 10, 10, -10),\n  c(cl[[3]]$y, -10, 10, 10),\n  col=scales::alpha(colGpe1, .9), density = param_dens,\n  border = NA\n)\n\ncontour_lwr &lt;- cl[sapply(cl, function(x) x$level == lvl[1])]\ncontour_upr &lt;- contours[sapply(cl, function(x) x$level == lvl[2])]\nfor (c in contour_lwr) lines(c$x, c$y, col = \"gray30\", lty = 1)\nfor (c in contour_upr) lines(c$x, c$y, col = \"gray30\", lty = 1)\n\n# Add contour lines\ncontour(u, u, z0, add = TRUE, lwd = 1, col = colGpe0, family = font_family, labcex = 1)\ncontour(u, u, z1, add = TRUE, lwd = 1, col = colGpe1, family = font_family, labcex = 1)\n\np &lt;- recordPlot()\npdf(paste0(path, \"gauss-ex-level-curves-overlap.pdf\"), width = 4.6, height = 4.6/2)\nreplayPlot(p)\ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\n\nFigure 2.1: Level curves of the two densities of \\(\\boldsymbol{X} \\mid A=a\\) in the toy example, and associated propensities. The white area correponds to \\(x\\) where the propensity score is in \\([1\\%,99\\%]\\). The blue area corresponds to \\(P[A=1 \\mid \\boldsymbol{X}] &lt; 1\\%\\), the red area corresponds to areas where \\(P[A=1 \\mid \\boldsymbol{X}] &gt; 99\\%\\).\n\n\n\n\n\n\n\n\n\n2.2.1 Propensity Scores\nLet us now estimate the ATT with AIPW. We first generate (again) some data, using the DGP presented in Section 2.1.\n\ndf &lt;- gen_data(\n  n = 500, \n  mu0 = -1, mu1 = +1, \n  r0 = +.7, r1 = -.5, a = 1, seed = 12345\n)\n\nLet us create a dataset, tb, with only the binary response (Y), the binary treatment (A), and the two covariates.\n\ntb &lt;- df[, c(\"Y\", \"A\", \"X1\", \"X2\")]\nS_name &lt;- \"A\"\nS_0 &lt;- 0\nY_name &lt;- \"Y\"\n\nWe will estimate the outcome model with a random forest, and the propensity score with a logistic model.\n\nlibrary(randomForest)\n\nWe use 5-fold cross-fitting.\n\nn_folds &lt;- 5 # 5-fold cross-fitting\nfolds &lt;- sample(rep(1:n_folds, length.out = n))\n# Init results\n## outcomes\nmu0_hat &lt;- rep(NA, n)\nmu1_hat &lt;- rep(NA, n)\n## propensity scores\ne_hat  &lt;- rep(NA, n)\n\nfor (k in 1:n_folds) {\n  idx_valid &lt;- which(folds == k)\n  idx_train &lt;- setdiff(1:n, idx_valid)\n  tb_train &lt;- tb |&gt; slice(idx_train)\n  tb_valid &lt;- tb |&gt; slice(-idx_train)\n  # Outcome models\n  mu0_model &lt;- randomForest(\n    x = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n      select(-!!Y_name, -!!S_name),\n    y = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n      pull(!!Y_name)\n  )\n  mu1_model &lt;- randomForest(\n    x = tb_train |&gt;\n      filter(!!sym(S_name) != !!S_0) |&gt; select(-!!Y_name, -!!S_name),\n    y = tb_train |&gt; filter(!!sym(S_name) != !!S_0) |&gt;\n      pull(!!Y_name)\n  )\n  \n  mu0_hat[idx_valid] &lt;- predict(\n    mu0_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n  ) |&gt; as.character() |&gt; as.numeric()\n  mu1_hat[idx_valid] &lt;- predict(\n    mu1_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n  ) |&gt; as.character() |&gt; as.numeric()\n  \n  # Propensity model\n  ps_model &lt;- glm(\n    formula(paste0(S_name, \"~.\")), data = tb_train |&gt; select(-!!Y_name),\n    family = binomial()\n  )\n  # Propensity scores\n  e_hat[idx_valid] &lt;- predict(\n    ps_model, newdata = tb_valid, type = \"response\"\n  )\n  \n}\n\nNow that we have estimated the propensity scores for each individual, we can have a look at their distribution.\n\nHistogramEstimated Density\n\n\nThe distribution of the propensity scores is shown in Figure 2.2\n\n\nCodes to create the Figure.\npar(mar = c(4.1, 4.1, 1.1, 1.1))\nhist(e_hat, xlab = \"Propensity scores\", main = \"\", family = font_family)\n\n\n\n\n\nFigure 2.2: Distribution of the estimated propensity scores.\n\n\n\n\n\n\n\n\n\n\nWe can also have a look at the density estimated with a Beta kernel (Figure 2.3).\n\n\nCodes to create the Figure.\nlibrary(kdensity)\nkprop &lt;- kdensity::kdensity(e_hat, kernel = \"beta\", bw = .05)\npar(mar = c(4.1, 4.1, 1.1, 1.1))\nplot(\n  kprop, xlab = \"Propensity Scores\", ylab = \"Density\", main = \"\",\n  ylim = c(0, 6), lwd = 2, family = font_family\n)\n\n\n\n\n\nFigure 2.3: Estimated density (beta kernel) of the propensity scores.\n\n\n\n\n\n\n\n\n\n\n\nWe compute the AIPW, according to Equation 2.1.\n\nS &lt;- pull(tb, !!S_name)\nY &lt;- pull(tb, !!Y_name)\ntreated_idx &lt;- which(S != S_0)\n\naipw_terms &lt;- S * (Y - mu0_hat) +\n  (1 - S) * (e_hat / (1 - e_hat)) * (Y - mu0_hat)\n(ATT_aipw &lt;- sum(aipw_terms[treated_idx]) / sum(S == 1))\n\n[1] 3.137477\n\n\n\n\n2.2.2 Decreasing Overlapping\nLet us now run some simulations in which we make the disttance between the two means of the covariates vary. Until now, this distance was \\(2\\), since \\(\\mu_0\\) was set to \\(-1\\) and \\(\\mu_1\\) was set to 1. Now, we randomly draw a shift value \\(\\alpha\\sim\\mathcal{U}(0,2)\\) and set the means as \\(\\mu_0 = \\alpha \\times (-1)\\) and \\(\\mu_1 = \\alpha \\times (+1)\\). The distance between the means of the two covariates now becomes equal to \\(2\\). All the other parameters of the DGP (see Section 2.1) remain unchanged.\nTo run the simulations, we create a wrapper function, simu(), that first generates 500 observations in each group, then proceeds to compute the ATT using the AIPW estimator (Equation 2.1)?\n\n\nThe function for the Monte-Carlo simulation.\nsimu &lt;- function(seed) {\n  \n  set.seed(seed)\n  n &lt;- 500\n  a &lt;- runif(1, min = 0, max = 2)\n  \n  df &lt;- gen_data(\n    n = n, \n    mu0 = -1, mu1 = +1, \n    r0 = +.7, r1 = -.5, a = a, seed = seed\n  )\n  \n  tb &lt;- df[, c(\"Y\", \"A\", \"X1\", \"X2\")]\n  \n  tb\n\n  S_name &lt;- \"A\"\n  S_0 &lt;- 0\n  Y_name &lt;- \"Y\"\n  \n  n_folds &lt;- 5 # 5-fold cross-fitting\n  folds &lt;- sample(rep(1:n_folds, length.out = n))\n  # Init results\n  ## outcomes\n  mu0_hat &lt;- rep(NA, n)\n  mu1_hat &lt;- rep(NA, n)\n  ## propensity scores\n  e_hat  &lt;- rep(NA, n)\n  \n  for (k in 1:n_folds) {\n    idx_valid &lt;- which(folds == k)\n    idx_train &lt;- setdiff(1:n, idx_valid)\n    tb_train &lt;- tb |&gt; slice(idx_train)\n    tb_valid &lt;- tb |&gt; slice(-idx_train)\n    # Outcome models\n    mu0_model &lt;- randomForest(\n      x = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n        select(-!!Y_name, -!!S_name),\n      y = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n        pull(!!Y_name)\n    )\n    mu1_model &lt;- randomForest(\n      x = tb_train |&gt;\n        filter(!!sym(S_name) != !!S_0) |&gt; select(-!!Y_name, -!!S_name),\n      y = tb_train |&gt; filter(!!sym(S_name) != !!S_0) |&gt;\n        pull(!!Y_name)\n    )\n    \n    mu0_hat[idx_valid] &lt;- predict(\n      mu0_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n    ) |&gt; as.character() |&gt; as.numeric()\n    mu1_hat[idx_valid] &lt;- predict(\n      mu1_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n    ) |&gt; as.character() |&gt; as.numeric()\n    \n    # Propensity model\n    ps_model &lt;- glm(\n      formula(paste0(S_name, \"~.\")), data = tb_train |&gt; select(-!!Y_name),\n      family = binomial()\n    )\n    # Propensity scores\n    e_hat[idx_valid] &lt;- predict(\n      ps_model, newdata = tb_valid, type = \"response\"\n    )\n    \n  }\n  \n  S &lt;- pull(tb, !!S_name)\n  Y &lt;- pull(tb, !!Y_name)\n  treated_idx &lt;- which(S != S_0)\n  \n  aipw_terms &lt;- S * (Y - mu0_hat) +\n    (1 - S) * (e_hat / (1 - e_hat)) * (Y - mu0_hat)\n  ATT_aipw &lt;- sum(aipw_terms[treated_idx]) / sum(S == 1)\n  \n  tibble(\n    a = a, \n    ATT_aipw = ATT_aipw, \n    prob_1_99 = mean((e_hat &gt; .01) & (e_hat &lt; .99))\n  )\n}\n\n\n\n# The estimation takes about 4 minutes.\n# Previously obtained results are loaded here (this chunk is not estimated).\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(randomForest)\n  library(mnormt)\n}) |&gt;\n  invisible()\n  \nclusterExport(cl, \"gen_data\")\n\nres_sim &lt;- pbapply::pblapply(1:2000, simu, cl = cl)\nstopCluster(cl)\n\nres_sim_tb &lt;- list_rbind(res_sim)\n\nsave(res_sim_tb, file = \"../output/res_sim_tb.rda\")\n\nWe load the previously obtained results:\n\nload(\"../output/res_sim_tb.rda\")\n\nThe results of the simulations are shown in Figure 2.4. Each dot represents a single replication. The left panel displays the probability that the estimated propensity scores lie within the interval \\([1\\%, 99\\%]\\), as a function of \\(\\alpha\\) (recall that the distance between the two covariates mean is \\(2\\alpha\\)). The orange curve is a spline showing the overall trend. The right panel shows the estimated Average Treatment Effect on the Treated using the AIPW estimator. The central orange line corresponds to a spline fit of the point estimates, while the lower and upper orange curves represent the 10th and 90th quantiles, respectively.\n\n\nCodes to create the Figure.\nlibrary(splines)\npar(mar = c(3.1, 3.1, 1.1, 1.1), mfrow = c(1, 2))\n\nplot(\n  res_sim_tb$a, res_sim_tb$prob_1_99,\n  cex = .3,\n  xlab = \"\",\n  ylab = \"\",\n  pch = 19,\n  col = scales::alpha(\"#009E73\", .1),\n  family = font_family\n)\ntitle(\n  xlab = latex2exp::TeX(\"Dist. btw. means ($\\\\alpha \\\\times 2$)\"),\n  # ylab = \"Prob. prop. score in [1%,99%]\",\n  ylab = latex2exp::TeX(\"$1\\\\%&lt;P[\\\\hat{e}(X)]&lt;99\\\\%$\"),\n  line = 2, family = font_family\n)\n\n# The proportions of propensity scores within [1%,99%] for each replication\n# and the drawn value for a\ntb_plot &lt;- data.frame(x = res_sim_tb$a, y = res_sim_tb$prob_1_99)\nrqxbar &lt;- lm(y ~ bs(x, 8), data = tb_plot)\nvu &lt;- (0:40) / 20\nyxbar &lt;- predict(rqxbar, newdata = data.frame(x = vu))\nlines(vu, yxbar, lwd = 2,col = \"#D55E00\")\n\n\nplot(\n  res_sim_tb$a, res_sim_tb$ATT_aipw,\n  cex = .3,\n  xlab = \"\",\n  ylab = \"\",\n  pch = 19,\n  col = scales::alpha(\"#009E73\", .1),\n  family = font_family\n)\ntitle(\n  xlab = latex2exp::TeX(\"Dist. btw. means ($\\\\alpha \\\\times 2$)\"), \n  # ylab = \"Estimated ATT, AIPW\", \n  ylab = latex2exp::TeX(\"$\\\\widehat{ATT}^{AIPW}$\"),\n  line = 2, family = font_family\n)\n\n# The AIPW for each replication, and the drawn value of the shift\ntb_aipw_a &lt;- tibble(x = res_sim_tb$a, y = res_sim_tb$ATT_aipw)\n# Quantile regressions at two levels: .9 and .1\nlibrary(quantreg)\nlibrary(splines)\nrq9 &lt;- rq(y ~ bs(x), data = tb_aipw_a, tau = .9)\nrq1 &lt;- rq(y ~ bs(x), data = tb_aipw_a, tau = .1)\nrqxbar &lt;- lm(y ~ bs(x), data = tb_aipw_a)\nvu &lt;- (0:40) / 20\ny1 &lt;- predict(rq1, newdata = data.frame(x = vu))\ny9 &lt;- predict(rq9, newdata = data.frame(x = vu))\nyxbar &lt;- predict(rqxbar, newdata = data.frame(x = vu))\nlines(vu, yxbar, lwd = 2, col = \"#D55E00\") # splines\nlines(vu, y1, lwd = 1, col = \"#D55E00\") # QR (tau=.1)\nlines(vu, y9, lwd = 1, col = \"#D55E00\") # QR (tau=.0)\nabline(h = 3, col = \"black\")\n\np &lt;- recordPlot()\npdf(paste0(path, \"gauss-ex-prop-1-99-alpha.pdf\"), width = 4.6, height = 4.6/2)\nreplayPlot(p)\ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\n\nFigure 2.4: Proportion of observation with a propensity score in \\([1\\%,99\\%]\\) (left) as a function of \\(\\alpha \\in[0,2]\\) ; estimation of the ATT (right), based on the AIPW estimator. Simulated toy example (\\(n=500\\)), 2,000 Monte-Carlo replications.",
    "crumbs": [
      "I. Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gaussian Example</span>"
    ]
  },
  {
    "objectID": "gaussian-example.html#brouillon",
    "href": "gaussian-example.html#brouillon",
    "title": "2  Gaussian Example",
    "section": "2.3 Brouillon",
    "text": "2.3 Brouillon\n\nset.seed(12345)\n# Parameters\nn &lt;- 500\nmu0 &lt;- -1\nmu1 &lt;- +1\nr0 &lt;- +.7\nr1 &lt;- -.5\na &lt;- 1\na0 &lt;-  3\na1 &lt;-  2\na2 &lt;-  -1.5\np1 &lt;- .5\nMu0 &lt;- rep(mu0, 2)\nMu1 &lt;- rep(mu1, 2)\nSig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\nSig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n\n# Draw covariates\nX0 &lt;- rmnorm(n, mean = a * Mu0, varcov = Sig0)\nX1 &lt;- rmnorm(n, mean = a * Mu1, varcov = Sig1)\n# Random noise\nE &lt;- rnorm(n)\n# Binary treatment\nA &lt;- sample(0:1, size = n, replace = TRUE, prob = c(1 - p1, p1))\n\ndf &lt;- tibble(\n  X1_0 = X0[, 1],\n  X2_0 = X0[, 2],\n  X1_1 = X1[, 1],\n  X2_1 = X1[, 2],\n  X1 = A * X1_1 + (1-A) * X1_0,\n  X2 = A * X2_1 + (1-A) * X2_0,\n  A = A,\n  Y0 = a1 * X1_0 + a2 * X2_0 + E,\n  Y1 = a1 * X1_1 + a2 * X2_1 + a0 + E,\n  Y = A * Y1 + (1-A) * Y0\n)\n\n\ntb &lt;- df[, c(\"Y\", \"A\", \"X1\", \"X2\")]\nS_name &lt;- \"A\"\nS_0 &lt;- 0\nY_name &lt;- \"Y\"\n\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\n\nn_folds &lt;- 5 # 5-fold cross-fitting\nfolds &lt;- sample(rep(1:n_folds, length.out = n))\n# Init results\n## outcomes\nmu0_hat &lt;- rep(NA, n)\nmu1_hat &lt;- rep(NA, n)\n## propensity scores\ne_hat  &lt;- rep(NA, n)\n\nfor (k in 1:n_folds) {\n  idx_valid &lt;- which(folds == k)\n  idx_train &lt;- setdiff(1:n, idx_valid)\n  tb_train &lt;- tb |&gt; slice(idx_train)\n  tb_valid &lt;- tb |&gt; slice(-idx_train)\n  # Outcome models\n  mu0_model &lt;- randomForest(\n    x = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n      select(-!!Y_name, -!!S_name),\n    y = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n      pull(!!Y_name)\n  )\n  mu1_model &lt;- randomForest(\n    x = tb_train |&gt;\n      filter(!!sym(S_name) != !!S_0) |&gt; select(-!!Y_name, -!!S_name),\n    y = tb_train |&gt; filter(!!sym(S_name) != !!S_0) |&gt;\n      pull(!!Y_name)\n  )\n  \n  mu0_hat[idx_valid] &lt;- predict(\n    mu0_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n  ) |&gt; as.character() |&gt; as.numeric()\n  mu1_hat[idx_valid] &lt;- predict(\n    mu1_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n  ) |&gt; as.character() |&gt; as.numeric()\n  \n  # Propensity model\n  ps_model &lt;- glm(\n    formula(paste0(S_name, \"~.\")), data = tb_train |&gt; select(-!!Y_name),\n    family = binomial()\n  )\n  # Propensity scores\n  e_hat[idx_valid] &lt;- predict(\n    ps_model, newdata = tb_valid, type = \"response\"\n  )\n  \n}\n\n\nlibrary(kdensity)\nkprop &lt;- kdensity::kdensity(e_hat, kernel = \"beta\", bw = .05)\nplot(kprop, xlab = \"\", main = \"\", ylab = \"\", ylim = c(0, 6), lwd = 2)\n\n\n\n\n\n\n\nS &lt;- pull(tb, !!S_name)\nY &lt;- pull(tb, !!Y_name)\ntreated_idx &lt;- which(S != S_0)\n\naipw_terms &lt;- S * (Y - mu0_hat) +\n  (1 - S) * (e_hat / (1 - e_hat)) * (Y - mu0_hat)\nATT_aipw &lt;- sum(aipw_terms[treated_idx]) / sum(S == 1)\n\n\n\nThe function for the Monte-Carlo simulation.\nsimu &lt;- function(n = 500) {\n  mu0 &lt;- -1\n  mu1 &lt;- +1\n  r0 &lt;- +.7\n  r1 &lt;- -.5\n  a &lt;- 2 * runif(1)\n  a0 &lt;- 3\n  a1 &lt;- 2\n  a2 &lt;- -1.5\n  Mu0 &lt;- rep(mu0, 2)\n  Mu1 &lt;- rep(mu1, 2)\n  Sig0 &lt;- matrix(c(1, r0, r0, 1), 2, 2)\n  Sig1 &lt;- matrix(c(1, r1, r1, 1), 2, 2)\n  # Draw covariates\n  X0 &lt;- rmnorm(n, mean = a * Mu0, varcov = Sig0)\n  X1 &lt;- rmnorm(n, mean = a * Mu1, varcov = Sig1)\n  # Random noise\n  E &lt;- rnorm(n)\n  # Binary treatment\n  p1 &lt;- .5\n  A &lt;- sample(0:1, size = n, replace = TRUE, prob = c(1 - p1, p1))\n  \n  df &lt;- tibble(\n    X1_0 = X0[, 1],\n    X2_0 = X0[, 2],\n    X1_1 = X1[, 1],\n    X2_1 = X1[, 2],\n    X1 = A * X1_1 + (1-A) * X1_0,\n    X2 = A * X2_1 + (1-A) * X2_0,\n    A = A,\n    Y0 = a1 * X1_0 + a2 * X2_0 + E,\n    Y1 = a1 * X1_1 + a2 * X2_1 + a0 + E,\n    Y = A * Y1 + (1-A) * Y0\n  )\n  \n  tb &lt;- df[, c(\"Y\", \"A\", \"X1\", \"X2\")]\n  \n  S_name &lt;- \"A\"\n  S_0 &lt;- 0\n  Y_name &lt;- \"Y\"\n  \n  n_folds &lt;- 5 # 5-fold cross-fitting\n  folds &lt;- sample(rep(1:n_folds, length.out = n))\n  # Init results\n  ## outcomes\n  mu0_hat &lt;- rep(NA, n)\n  mu1_hat &lt;- rep(NA, n)\n  ## propensity scores\n  e_hat  &lt;- rep(NA, n)\n  \n  for (k in 1:n_folds) {\n    idx_valid &lt;- which(folds == k)\n    idx_train &lt;- setdiff(1:n, idx_valid)\n    tb_train &lt;- tb |&gt; slice(idx_train)\n    tb_valid &lt;- tb |&gt; slice(-idx_train)\n    # Outcome models\n    mu0_model &lt;- randomForest(\n      x = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n        select(-!!Y_name, -!!S_name),\n      y = tb_train |&gt; filter(!!sym(S_name) == !!S_0) |&gt;\n        pull(!!Y_name)\n    )\n    mu1_model &lt;- randomForest(\n      x = tb_train |&gt;\n        filter(!!sym(S_name) != !!S_0) |&gt; select(-!!Y_name, -!!S_name),\n      y = tb_train |&gt; filter(!!sym(S_name) != !!S_0) |&gt;\n        pull(!!Y_name)\n    )\n    \n    mu0_hat[idx_valid] &lt;- predict(\n      mu0_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n    ) |&gt; as.character() |&gt; as.numeric()\n    mu1_hat[idx_valid] &lt;- predict(\n      mu1_model, newdata = tb_valid |&gt; select(-!!Y_name, -!!S_name)\n    ) |&gt; as.character() |&gt; as.numeric()\n    \n    # Propensity model\n    ps_model &lt;- glm(\n      formula(paste0(S_name, \"~.\")), data = tb_train |&gt; select(-!!Y_name),\n      family = binomial()\n    )\n    # Propensity scores\n    e_hat[idx_valid] &lt;- predict(\n      ps_model, newdata = tb_valid, type = \"response\"\n    )\n    \n  }\n  \n  S &lt;- pull(tb, !!S_name)\n  Y &lt;- pull(tb, !!Y_name)\n  treated_idx &lt;- which(S != S_0)\n  \n  aipw_terms &lt;- S * (Y - mu0_hat) +\n    (1 - S) * (e_hat / (1 - e_hat)) * (Y - mu0_hat)\n  ATT_aipw &lt;- sum(aipw_terms[treated_idx]) / sum(S == 1)\n  \n  tibble(\n    a = a, \n    ATT_aipw = ATT_aipw, \n    prob_1_99 = mean((e_hat &gt; .01) & (e_hat &lt; .99))\n  )\n}\n\n\n\n# The estimation takes about 4 minutes.\n# Previously obtained results are loaded here (this chunk is not estimated).\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(randomForest)\n  library(mnormt)\n}) |&gt;\n  invisible()\n\n# V &lt;- Vectorize(simu)(rep(500,2))\nres_sim &lt;- pbapply::pblapply(rep(500, 2000), simu, cl = cl)\nstopCluster(cl)\n\nres_sim_tb &lt;- list_rbind(res_sim)\n\nsave(res_sim_tb, file = \"../output/res_sim_tb.rda\")\n\nWe load the previously obtained results:\n\nload(\"../output/res_sim_tb.rda\")\n\n\nplot(\n  res_sim_tb$a, res_sim_tb$ATT_aipw,\n  cex = 1,\n  xlab = \"Distance between means (x2)\",\n  ylab = \"Estimated ATT, AIPW, (n=500)\",\n  pch = 19,\n  col = scales::alpha(\"blue\", .3)\n)\nDF &lt;- data.frame(x = res_sim_tb$a, y = res_sim_tb$ATT_aipw)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\nlibrary(splines)\nrq9 &lt;- rq(y ~ bs(x), data = DF, tau = .9)\nrq1 &lt;- rq(y ~ bs(x), data = DF, tau = .1)\nrqxbar &lt;- lm(y ~ bs(x), data = DF)\nvu &lt;- (0:40) / 20\ny1 &lt;- predict(rq1, newdata = data.frame(x = vu))\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.000491942744702101, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\ny9 &lt;- predict(rq9, newdata = data.frame(x = vu))\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.000491942744702101, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\nyxbar &lt;- predict(rqxbar, newdata = data.frame(x = vu))\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.000491942744702101, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\nlines(vu, yxbar, lwd = 2, col = \"red\")\nlines(vu, y1, lwd = 1, col = \"red\")\nlines(vu, y9, lwd = 1, col = \"red\")\nabline(h = 3, col = \"blue\")\n\n\n\n\n\n\n\n\n\nplot(\n  res_sim_tb$a, res_sim_tb$prob_1_99,\n  cex = 1,\n  xlab = \"Distance between means (x2)\",\n  ylab = \"Probability propensity score in [1%;99%]\",\n  pch = 19,\n  col = scales::alpha(\"blue\", .3)\n)\nDF2 &lt;- data.frame(x = res_sim_tb$a, y = res_sim_tb$prob_1_99)\nrqxbar &lt;- lm(y ~ bs(x, 8), data = DF2)\nvu &lt;- (0:40) / 20\nyxbar &lt;- predict(rqxbar, newdata = data.frame(x = vu))\n\nWarning in bs(x, degree = 3L, knots = c(0.339898149405296, 0.696911755638818, :\nsome 'x' values beyond boundary knots may cause ill-conditioned bases\n\nlines(vu, yxbar, lwd = 2,col = \"red\")",
    "crumbs": [
      "I. Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Gaussian Example</span>"
    ]
  },
  {
    "objectID": "transport-categ-toy.html#random-matching",
    "href": "transport-categ-toy.html#random-matching",
    "title": "5  Transporting a Categorical Variable",
    "section": "5.2 Random Matching",
    "text": "5.2 Random Matching\nThere are \\(6!=720\\) different random matching that can be done. We will show two of them below.\n\n5.2.1 First Random Matching\nLet us first consider a random matching in which the individuals matched are 1 (group 0) and 12 (group 1), 2 and 9, 3 and 7, 4 and 10, 5 and 8, 6 and 12. We can compute the difference \\(y(1) - y(0)\\) for each pair of matched individuals (column diff in the table below).\n\nmatched_ex_1 &lt;- tribble(\n  ~i_0, ~i_1,\n  1, 12,\n  2, 9,\n  3, 7,\n  4, 10,\n  5, 8,\n  6, 11\n) |&gt;\n  left_join(\n    group_0 |&gt;\n      rename_with(~str_c(.x, \"_0\")),\n    by = \"i_0\"\n  ) |&gt;\n  left_join(\n    group_1 |&gt;\n      rename_with(~str_c(.x, \"_1\")),\n    by = \"i_1\"\n  )\n\ntb_att_1 &lt;- \n  matched_ex_1 |&gt;\n  mutate(diff = y_1 - y_0) |&gt;\n  select(i_0, i_1, diff, x_0, x_1)\ntb_att_1\n\n# A tibble: 6 × 5\n    i_0   i_1  diff x_0   x_1  \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1    12     7 A     C    \n2     2     9     3 A     B    \n3     3     7     0 A     A    \n4     4    10     2 B     B    \n5     5     8    -1 B     B    \n6     6    11     1 C     C    \n\n\nThe average treatment on the treated is thus, in that case:\n\nmean(tb_att_1$diff)\n\n[1] 2\n\n\nAnd if we compute the ATT by category of \\(X\\):\n\ntb_att_1 |&gt; group_by(x_1) |&gt; summarise(ATT = mean(diff))\n\n# A tibble: 3 × 2\n  x_1     ATT\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      0   \n2 B      1.33\n3 C      4   \n\n\n\n\n5.2.2 Second Random Matching\nWe can consider, for the sake of illustration, a second random matching, where the individuals matched are 1 and 8, 2 and 7, 3 and 11, 4 and 10, 5 and 9, 6 and 12. Again, we can compute the difference in outcomes \\(y(1)-y(0)\\) for each pair of matched individuals.\n\nmatched_ex_2 &lt;- tribble(\n  ~i_0, ~i_1,\n  1, 8,\n  2, 7,\n  3, 11,\n  4, 10,\n  5, 9,\n  6, 12\n) |&gt;\n  left_join(\n    group_0 |&gt;\n      rename_with(~str_c(.x, \"_0\")),\n    by = \"i_0\"\n  ) |&gt;\n  left_join(\n    group_1 |&gt;\n      rename_with(~str_c(.x, \"_1\")),\n    by = \"i_1\"\n  )\n\ntb_att_2 &lt;- matched_ex_2 |&gt;\n  mutate(diff = y_1 - y_0) |&gt;\n  select(i_0, i_1, diff, x_0, x_1)\ntb_att_2\n\n# A tibble: 6 × 5\n    i_0   i_1  diff x_0   x_1  \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1     8     3 A     B    \n2     2     7     1 A     A    \n3     3    11     4 A     C    \n4     4    10     2 B     B    \n5     5     9     0 B     B    \n6     6    12     2 C     C    \n\n\nThe average treatment on the treated is thus, in that case:\n\nmean(tb_att_2$diff)\n\n[1] 2\n\n\nAnd if we compute the ATT by category of \\(X\\):\n\ntb_att_2 |&gt; group_by(x_1) |&gt; summarise(ATT = mean(diff))\n\n# A tibble: 3 × 2\n  x_1     ATT\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      1   \n2 B      1.67\n3 C      3",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transporting a Categorical Variable</span>"
    ]
  },
  {
    "objectID": "transport-categ-toy.html#matching-based-on-euclidean-distances",
    "href": "transport-categ-toy.html#matching-based-on-euclidean-distances",
    "title": "5  Transporting a Categorical Variable",
    "section": "5.3 Matching Based on Euclidean Distances",
    "text": "5.3 Matching Based on Euclidean Distances\nLet us nw perform optimal matching between individuals from the two groups using Euclidean distances. These distances are computed in the space of centered log-ratio (clr) transformed probability vectors associated with each individual’s membership in one of the classes.\nLet us first extract the probability vectors (\\(p_A, p_B, p_C\\)) from both groups and apply the clr transformation to make them suitable for Euclidean geometry in the compositional space.\n\nlibrary(compositions)\nall_coords &lt;- rbind(\n  as.matrix(group_0[, c(\"p_A\", \"p_B\", \"p_C\")]),\n  as.matrix(group_1[, c(\"p_A\", \"p_B\", \"p_C\")])\n) |&gt; \n  clr()\n\nThen, we can compute the pairwaise Euclidean distances between individuals in group 1 and those in group 0, based on their clr-transformed probability vectors.\n\nrow.names(all_coords) &lt;- c(group_0$i, group_1$i)\n# Euclidean distances between the clr transform of the propensities\nD &lt;- as.matrix(dist(all_coords, method = \"euclidean\"))\nn0 &lt;- nrow(group_0)\nn1 &lt;- nrow(group_1)\nbetween_distances &lt;- D[(n0 + 1):(n0 + n1), 1:n0]\nround(between_distances, 2)\n\n      1    2    3    4    5    6\n7  0.63 0.00 1.30 1.77 1.38 2.75\n8  2.91 2.42 2.67 0.98 1.30 1.77\n9  0.80 0.64 0.79 1.78 1.46 2.16\n10 2.27 1.85 1.93 1.06 1.15 1.36\n11 1.99 2.09 0.98 2.67 2.53 1.30\n12 2.92 2.67 2.21 2.09 2.21 0.41\n\n\nWe aim to find the optimal matching between the individuals in group 1 and group 0 based on these distances. Formally, we want to solve the following optimal transport problem: \\[\n\\min_{P\\in\\mathcal{U}(\\boldsymbol{1}_{n_1},\\boldsymbol{1}_{n_0})}\n\\langle P,\\,C\\rangle,\n\\] where \\(C:=[C_{i,j}]\\) is the cost matrix, with \\(C_{ij}\\) measuring the cost of matching individual \\(i\\) from group 1 to individual \\(j\\) from group 0. Here, we use the Euclidean distance that we juste computed. The total cost is given by \\(\\langle P, C\\rangle=\\sum_{i=1}^{n_1}\\sum_{j=1}^{n_0}P_{ij}\\,C_{ij}\\). The set of admissible transport plans is defined as follows: \\[\n\\left\\{\\,P\\in\\mathbb{R}_+^{n_1\\times n_0}:\nP\\,\\mathbf{1}_{n_0}=\\frac{\\mathbf{1}_{n_1}}{n_1},\\\nP^\\top\\mathbf{1}_{n_1}=\\frac{\\mathbf{1}_{n_0}}{n_0}\n\\right\\}.\n\\] We thus have a uniform mass distribution across both groups.\n\n\n\n\n\n\nNote\n\n\n\nAn alternative cost function (which is not used here) is the cross-entropy between two compositional vectors: \\[\n\\begin{equation*}\nc(\\mathbf{x},\\mathbf{y})=\\log\\left(\\frac{1}{d}\\sum_{i=1}^d\\frac{y_i}{x_i}\\right)-\\frac{1}{d}\\sum_{i=1}^d\\log\\left(\\frac{y_i}{x_i}\\right),\n\\end{equation*}.\n\\] This corresponds to the “Dirichlet transport” (Baxendale and Wong (2022)).\n\n\nWe solve the optimal transport problem using the transport() function from the {transport} package. This function computes the optimal matching plan based on the cost matrix.\n\n# source weights\nmass_source &lt;- rep(1 / n1, n1)\n# target weights\nmass_target &lt;- rep(1 / n0, n0)\n\n# Solve the optimal transport plan\not_plan &lt;- transport::transport(\n  a = mass_source, b = mass_target, costm = between_distances, \n  method = \"networkflow\"\n)\not_plan$i_0 &lt;- group_0$i[ot_plan$to]\not_plan$i_1 &lt;- group_1$i[ot_plan$from]\not_plan\n\n  from to      mass i_0 i_1\n1    1  2 0.1666667   2   7\n2    2  4 0.1666667   4   8\n3    3  1 0.1666667   1   9\n4    4  5 0.1666667   5  10\n5    5  3 0.1666667   3  11\n6    6  6 0.1666667   6  12\n\n\nWe can visualize the results in a ternary plot (Figure 5.1). The lines depict the matched individuals (shown by dots and their index).\n\n\nCodes to create the Figure.\nall_data &lt;- \n  bind_rows(group_0 |&gt; mutate(group = \"0\"), group_1 |&gt; mutate(group = \"1\"))\n\nlibrary(ggtern)\n\np &lt;- ggtern(\n  data = all_data |&gt; \n    left_join(\n      ot_plan |&gt; \n        mutate(\n          i_0 = as.numeric(i_0),\n          i_1 = as.numeric(i_1),\n          id_match = as.character(row_number())\n        ) |&gt; \n        dplyr::select(i_0, i_1, id_match) |&gt; \n        pivot_longer(cols = c(i_0, i_1), values_to = \"i\") |&gt; \n        dplyr::select(-name)\n    ),\n  mapping = aes(x = p_A, y = p_B, z = p_C, group = id_match)\n) +\n  geom_point(mapping = aes(shape = group, colour = x), size = 4) +\n  geom_text(\n    mapping = aes(\n      label = i, \n      x = p_A + ifelse(group == 0, -1, 1) * 0.05\n    ),\n    size = .3*font_size\n  ) +\n  labs(x = \"$p_A$\", y = \"$p_B$\", z = \"$p_C\") +\n  geom_line(\n    colour = \"gray40\", \n    # mapping = aes(linetype = id_match)\n  ) +\n  scale_colour_manual(name = \"category\", values = col_categ) +\n  scale_shape_discrete(name = \"group\") +\n  theme_light(base_size = font_size, base_family = font_family) +\n  # theme_paper() +\n  theme_ggtern() +\n  theme(\n    legend.title = element_text(size = .8*font_size),\n    legend.text = element_text(size = .8*font_size)\n  ) +\n  theme_latex(TRUE) +\n  theme_hidetitles()\n\np\n\n\n\n\n\nFigure 5.1: 1-to-1 Matching with optimal transport based on the distances between the individuals with respect to their estimated probabilities of being in each class.\n\n\n\n\n\n\n\n\n\n\nCodes to export the figure in PDF.\nfilename &lt;- \"ternary-toy\"\nggsave(\n  p, file = str_c(path, filename, \".pdf\"),\n  height = 2.2*1.75, width = 4*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))\n\n\nWe can compute the differences \\(y(1)-y(0)\\) for each matched individuals.\n\not_plan_diff &lt;- \n  ot_plan |&gt; \n  left_join(group_0 |&gt; select(i_0 = i, y_0 = y, x_0 = x), by = \"i_0\") |&gt; \n  left_join(group_1 |&gt; select(i_1 = i, y_1 = y, x_1 = x), by = \"i_1\") |&gt; \n  mutate(diff = y_1 - y_0) |&gt;\n  select(i_0, i_1, diff, x_0, x_1)\not_plan_diff\n\n  i_0 i_1 diff x_0 x_1\n1   2   7    1   A   A\n2   4   8    0   B   B\n3   1   9    4   A   B\n4   5  10    1   B   B\n5   3  11    4   A   C\n6   6  12    2   C   C\n\n\nResulting in an ATT of:\n\nmean(ot_plan_diff$diff)\n\n[1] 2\n\n\nAnd if we compute the ATT by category of \\(X\\):\n\not_plan_diff |&gt; group_by(x_1) |&gt; summarise(ATT = mean(diff))\n\n# A tibble: 3 × 2\n  x_1     ATT\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      1   \n2 B      1.67\n3 C      3   \n\n\n\n\n\n\nBaxendale, Peter, and Ting-Kam Leonard Wong. 2022. “Random Concave Functions.” The Annals of Applied Probability 32 (2): 812–52.",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transporting a Categorical Variable</span>"
    ]
  },
  {
    "objectID": "transport-categ-toy.html#sec-matching-euclidean",
    "href": "transport-categ-toy.html#sec-matching-euclidean",
    "title": "5  Transporting a Categorical Variable",
    "section": "5.3 Matching Using the Euclidean Distances Cost Matrix",
    "text": "5.3 Matching Using the Euclidean Distances Cost Matrix\nLet us nw perform optimal matching between individuals from the two groups using Euclidean distances. These distances are computed in the space of centered log-ratio (clr) transformed probability vectors associated with each individual’s membership in one of the classes.\nLet us first extract the probability vectors (\\(p_A, p_B, p_C\\)) from both groups and apply the clr transformation to make them suitable for Euclidean geometry in the compositional space.\n\nlibrary(compositions)\nall_coords &lt;- rbind(\n  as.matrix(group_0[, c(\"p_A\", \"p_B\", \"p_C\")]),\n  as.matrix(group_1[, c(\"p_A\", \"p_B\", \"p_C\")])\n) |&gt; \n  clr()\n\nThen, we can compute the pairwise Euclidean distances between individuals in group 1 and those in group 0, based on their clr-transformed probability vectors.\n\nrow.names(all_coords) &lt;- c(group_0$i, group_1$i)\n# Euclidean distances between the clr transform of the propensities\nD &lt;- as.matrix(dist(all_coords, method = \"euclidean\"))\nn0 &lt;- nrow(group_0)\nn1 &lt;- nrow(group_1)\nbetween_distances &lt;- D[(n0 + 1):(n0 + n1), 1:n0]\nround(between_distances, 2)\n\n      1    2    3    4    5    6\n7  0.63 0.00 1.30 1.77 1.38 2.75\n8  2.91 2.42 2.67 0.98 1.30 1.77\n9  0.80 0.64 0.79 1.78 1.46 2.16\n10 2.27 1.85 1.93 1.06 1.15 1.36\n11 1.99 2.09 0.98 2.67 2.53 1.30\n12 2.92 2.67 2.21 2.09 2.21 0.41\n\n\nWe aim to find the optimal matching between the individuals in group 1 and group 0 based on these distances. Formally, we want to solve the following optimal transport problem: \\[\n\\min_{P\\in\\mathcal{U}(\\boldsymbol{1}_{n_1},\\boldsymbol{1}_{n_0})}\n\\langle P,\\,C\\rangle,\n\\] where \\(C:=[C_{i,j}]\\) is the cost matrix, with \\(C_{ij}\\) measuring the cost of matching individual \\(i\\) from group 1 to individual \\(j\\) from group 0. Here, we use the Euclidean distance that we juste computed. The total cost is given by \\(\\langle P, C\\rangle=\\sum_{i=1}^{n_1}\\sum_{j=1}^{n_0}P_{ij}\\,C_{ij}\\). The set of admissible transport plans is defined as follows: \\[\n\\left\\{\\,P\\in\\mathbb{R}_+^{n_1\\times n_0}:\nP\\,\\mathbf{1}_{n_0}=\\frac{\\mathbf{1}_{n_1}}{n_1},\\\nP^\\top\\mathbf{1}_{n_1}=\\frac{\\mathbf{1}_{n_0}}{n_0}\n\\right\\}.\n\\] We thus have a uniform mass distribution across both groups.\n\n\n\n\n\n\nNote\n\n\n\nAn alternative cost function (which is not used here) is the cross-entropy between two compositional vectors: \\[\n\\begin{equation*}\nc(\\mathbf{x},\\mathbf{y})=\\log\\left(\\frac{1}{d}\\sum_{i=1}^d\\frac{y_i}{x_i}\\right)-\\frac{1}{d}\\sum_{i=1}^d\\log\\left(\\frac{y_i}{x_i}\\right),\n\\end{equation*}.\n\\] This corresponds to the “Dirichlet transport” (Baxendale and Wong (2022)). This alternative is considered below, in Section 5.4.\n\n\nWe solve the optimal transport problem using the transport() function from the {transport} package. This function computes the optimal matching plan based on the cost matrix.\n\n# source weights\nmass_source &lt;- rep(1 / n1, n1)\n# target weights\nmass_target &lt;- rep(1 / n0, n0)\n\n# Solve the optimal transport plan\not_plan &lt;- transport::transport(\n  a = mass_source, b = mass_target, costm = between_distances, \n  method = \"networkflow\"\n)\not_plan$i_0 &lt;- group_0$i[ot_plan$to]\not_plan$i_1 &lt;- group_1$i[ot_plan$from]\not_plan\n\n  from to      mass i_0 i_1\n1    1  2 0.1666667   2   7\n2    2  4 0.1666667   4   8\n3    3  1 0.1666667   1   9\n4    4  5 0.1666667   5  10\n5    5  3 0.1666667   3  11\n6    6  6 0.1666667   6  12\n\n\nWe can visualize the results in a ternary plot (Figure 5.1). The lines depict the matched individuals (shown by dots and their index).\n\n\nCodes to create the Figure.\nall_data &lt;- \n  bind_rows(group_0 |&gt; mutate(group = \"0\"), group_1 |&gt; mutate(group = \"1\"))\n\nlibrary(ggtern)\n\np &lt;- ggtern(\n  data = all_data |&gt; \n    left_join(\n      ot_plan |&gt; \n        mutate(\n          i_0 = as.numeric(i_0),\n          i_1 = as.numeric(i_1),\n          id_match = as.character(row_number())\n        ) |&gt; \n        dplyr::select(i_0, i_1, id_match) |&gt; \n        pivot_longer(cols = c(i_0, i_1), values_to = \"i\") |&gt; \n        dplyr::select(-name)\n    ),\n  mapping = aes(x = p_A, y = p_B, z = p_C, group = id_match)\n) +\n  geom_point(mapping = aes(shape = group, colour = x), size = 4) +\n  geom_text(\n    mapping = aes(\n      label = i, \n      x = p_A + ifelse(group == 0, -1, 1) * 0.05\n    ),\n    size = .3*font_size\n  ) +\n  labs(x = \"$p_A$\", y = \"$p_B$\", z = \"$p_C\") +\n  geom_line(\n    colour = \"gray40\", \n    # mapping = aes(linetype = id_match)\n  ) +\n  scale_colour_manual(name = \"category\", values = col_categ) +\n  scale_shape_discrete(name = \"group\") +\n  theme_light(base_size = font_size, base_family = font_family) +\n  # theme_paper() +\n  theme_ggtern() +\n  theme(\n    legend.title = element_text(size = .8*font_size),\n    legend.text = element_text(size = .8*font_size)\n  ) +\n  theme_latex(TRUE) +\n  theme_hidetitles()\n\np\n\n\n\n\n\nFigure 5.1: 1-to-1 Matching with optimal transport based on the distances between the individuals with respect to their estimated probabilities of being in each class.\n\n\n\n\n\n\n\n\n\n\nCodes to export the figure in PDF.\nfilename &lt;- \"ternary-toy\"\nggsave(\n  p, file = str_c(path, filename, \".pdf\"),\n  height = 2.2*1.75, width = 4*1.75,\n  family = font_family,\n  device = cairo_pdf\n)\n# Crop PDF\nsystem(paste0(\"pdfcrop \", path, filename, \".pdf \", path, filename, \".pdf\"))\n\n\nWe can compute the differences \\(y(1)-y(0)\\) for each matched individuals.\n\not_plan_diff &lt;- \n  ot_plan |&gt; \n  left_join(group_0 |&gt; select(i_0 = i, y_0 = y, x_0 = x), by = \"i_0\") |&gt; \n  left_join(group_1 |&gt; select(i_1 = i, y_1 = y, x_1 = x), by = \"i_1\") |&gt; \n  mutate(diff = y_1 - y_0) |&gt;\n  select(i_0, i_1, diff, x_0, x_1)\not_plan_diff\n\n  i_0 i_1 diff x_0 x_1\n1   2   7    1   A   A\n2   4   8    0   B   B\n3   1   9    4   A   B\n4   5  10    1   B   B\n5   3  11    4   A   C\n6   6  12    2   C   C\n\n\nResulting in an ATT of:\n\nmean(ot_plan_diff$diff)\n\n[1] 2\n\n\nAnd if we compute the ATT by category of \\(X\\):\n\not_plan_diff |&gt; group_by(x_1) |&gt; summarise(ATT = mean(diff))\n\n# A tibble: 3 × 2\n  x_1     ATT\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      1   \n2 B      1.67\n3 C      3",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transporting a Categorical Variable</span>"
    ]
  },
  {
    "objectID": "transport-categ-toy.html#sec-matching-ce",
    "href": "transport-categ-toy.html#sec-matching-ce",
    "title": "5  Transporting a Categorical Variable",
    "section": "5.4 Matching Using the Cross-Entropy Cost Matrix",
    "text": "5.4 Matching Using the Cross-Entropy Cost Matrix\nIn Section 5.3, we used the Euclidean distance of the clr-transformed vector of probabilities as the cost function to solve the optimal transport problem. Here, we consider an alternative cost function, the cross-entreopy: \\[\nc(\\mathbf{x}, \\mathbf{y}) = \\log\\left(\\frac{1}{d} \\sum_{i=1}^d \\frac{y_i}{x_i}\\right) - \\frac{1}{d} \\sum_{i=1}^d \\log\\left(\\frac{y_i}{x_i}\\right).\n\\]\nWe first extract the probability vectors for the individuals from both groups (without clr transform), and we make sure there is no probability equal to 0.\n\np0 &lt;- as.matrix(group_0[, c(\"p_A\", \"p_B\", \"p_C\")])\np1 &lt;- as.matrix(group_1[, c(\"p_A\", \"p_B\", \"p_C\")])\np0 &lt;- pmax(p0, 1e-10)\np1 &lt;- pmax(p1, 1e-10)\n\nLet us define the cross-entropy cost function:\n\ncross_entropy_cost &lt;- function(x, y) {\n  d &lt;- length(x)\n  log(mean(y / x)) - mean(log(y / x))\n}\n\nWe can then compute the pairwise cost matrix (group1 rows vs group0 columns).\n\nbetween_distances_ce &lt;- outer(\n  1:nrow(p1), 1:nrow(p0),\n  Vectorize(function(i, j) cross_entropy_cost(p1[i, ], p0[j, ]))\n)\nround(between_distances_ce, 4)\n\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]\n[1,] 0.0598 0.0000 0.2894 0.4670 0.3057 0.9985\n[2,] 1.2447 0.9537 0.8514 0.1542 0.2894 0.4670\n[3,] 0.1149 0.0619 0.0959 0.5474 0.3836 0.6215\n[4,] 0.8534 0.5984 0.5067 0.1610 0.1813 0.3289\n[5,] 0.5154 0.4892 0.1542 1.0435 0.8708 0.2379\n[6,] 1.3118 1.0435 0.8232 0.4892 0.5436 0.0266\n\n\nThen, we can solde the optimal transport problem.\n\not_plan_ce &lt;- transport::transport(\n  a = mass_source,\n  b = mass_target,\n  costm = between_distances_ce,\n  method = \"networkflow\"\n)\not_plan_ce$i_0 &lt;- group_0$i[ot_plan_ce$to]\not_plan_ce$i_1 &lt;- group_1$i[ot_plan_ce$from]\not_plan_ce\n\n  from to      mass i_0 i_1\n1    1  2 0.1666667   2   7\n2    2  4 0.1666667   4   8\n3    3  1 0.1666667   1   9\n4    4  5 0.1666667   5  10\n5    5  3 0.1666667   3  11\n6    6  6 0.1666667   6  12\n\n\nWe can compute the differences \\(y(1)-y(0)\\) for each matched individuals.\n\not_plan_ce_diff &lt;- \n  ot_plan_ce |&gt; \n  left_join(group_0 |&gt; select(i_0 = i, y_0 = y, x_0 = x), by = \"i_0\") |&gt; \n  left_join(group_1 |&gt; select(i_1 = i, y_1 = y, x_1 = x), by = \"i_1\") |&gt; \n  mutate(diff = y_1 - y_0) |&gt;\n  select(i_0, i_1, diff, x_0, x_1)\not_plan_ce_diff\n\n  i_0 i_1 diff x_0 x_1\n1   2   7    1   A   A\n2   4   8    0   B   B\n3   1   9    4   A   B\n4   5  10    1   B   B\n5   3  11    4   A   C\n6   6  12    2   C   C\n\n\nResulting in an ATT of:\n\nmean(ot_plan_ce_diff$diff)\n\n[1] 2\n\n\nAnd if we compute the ATT by category of \\(X\\):\n\not_plan_ce_diff |&gt; group_by(x_1) |&gt; summarise(ATT = mean(diff))\n\n# A tibble: 3 × 2\n  x_1     ATT\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      1   \n2 B      1.67\n3 C      3   \n\n\n\n\n\n\nBaxendale, Peter, and Ting-Kam Leonard Wong. 2022. “Random Concave Functions.” The Annals of Applied Probability 32 (2): 812–52.",
    "crumbs": [
      "II. Sequential Transport for General Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Transporting a Categorical Variable</span>"
    ]
  }
]