---
title: "Treatment Effect"
format: 
  html:
    toc: true
    number-sections: true
    number-depth: 3
    embed-resources: true
bibliography: ./biblio.bib
editor: source
---

```{r}
library(dplyr)
library(randomForest)
set.seed(1234)
```

Let $Y$ be an outcome. Let $\boldsymbol{X}\in \mathcal{X}^p$ be a matrix of $p$ characteristics, and let $T\in\{0,1\}$ denote the treatment, with $T=1$ for treated and $T=0$ for control.

Let $Y(0)$ and $Y(1)$ denote the outcome if not treated (control) and if treated, respectively.


# Data

```{r generate-data}
n <- 1000
X1 <- rnorm(n)
X2star <- X1 + rnorm(n)
X2 <- cut(X2star, c(-Inf, -1, +1, +Inf) * .7, c("A" ,"B" ,"C"))
X3 <- X1 + (X2 == "B") + rnorm(n)
eta <- X1 + (X2 == "A") - 0.4
probaT <- exp(eta) / (1 + exp(eta))
T <- rbinom(n, size=1, prob = probaT)
Y <- 1 + 2 * X1 + rnorm(n) + T * (X3 + (X2 == "C"))
base <- tibble(
  X1 = X1,
  X2 = as.factor(X2),
  X3 = X3,
  T  = T,
  Y = Y
)
```



Train/Test splits:
```{r train-test}
train_idx <- sample(seq_len(n), size = floor(0.8 * n))
base_train <- base[train_idx, ]
base_test  <- base[-train_idx, ]
```

One-hot encoding:

```{r one-hot}
X_train <- model.matrix(~ X1 + X2 + X3 - 1, data = base_train)
X_test  <- model.matrix(~ X1 + X2 + X3 - 1, data = base_test)
```

Some objects to identify treated/control in each data set:
```{r define-ind_train_0}
ind_train_0 <- base_train$T == 0
ind_train_1 <- base_train$T == 1
ind_test_0 <- base_test$T == 0
ind_test_1 <- base_test$T == 1
```


# ATE

The **Average Treatment Effect** (ATE) is defined as:

$$
\text{ATE} = \mathbb{E}[Y(1) - Y(0)].
$${#eq-ate}


Under the assumption of unconfoundedness, $(Y(0), Y(1)) \perp T \mid \boldsymbol{X}$, and overlap, $0 < \mathbb{P}(T = 1 \mid X) < 1$, the ATE can be identified from observational data $\{(Y_i, \boldsymbol{X}_i, T_i)\}_{i=1}^{n}$. We can estimate it using Ordinary Least Squares (OLS), by simply regressing the outcome $Y$ on the treatment $T$ alone:

$$
Y_i = \alpha + \tau T_i + \varepsilon_i,
$$

where $\hat{\tau}$ is an unbiased estimator of the ATE if treatment is randomly assigned. When covariates $\boldsymbol{X}$ are included to adjust for confounding, the regression becomes:

$$
Y_i = \alpha + \tau T_i + X_i^\top \beta + \varepsilon_i.
$$

## OLS


```{r ate_lm}
lm_ate <- lm(Y ~ T + X1 + X2 + X3, data = base_train)
(ate_lm <- coef(lm_ate)[["T"]])
```

## IPW

With the **Inverse Probability Weighting** (IPW) estimator, observations are reweighted to recreate a pseudo-population where the treatment is independent of the covariates $\boldsymbol{X}$. The following unconfoundedness assumption is required:
$$
(Y(0), Y(1)) \perp T \mid X
$$

The weights are based on the **propensity scores**, $e(X) = \mathbb{P}(T = 1 \mid X)$. They differ according to the treatment:

* for treated: $1/e(X)$,
* for control: $1/(1 - e(X))$.

The ATE estimated with IPW writes:
$$
\widehat{\text{ATE}}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^n \left( \frac{T_i Y_i}{\hat{e}(X_i)} - \frac{(1 - T_i) Y_i}{1 - \hat{e}(X_i)} \right)
$$
```{r}
# Propensity scores
ps_model <- glm(T ~ X1 + X2 + X3, data = base_train, family = binomial)
e_hat <- predict(ps_model, newdata = base_test, type = "response")

# Weights
w_treated <- base_test$T / e_hat
w_control <- (1 - base_test$T) / (1 - e_hat)

# ATE with IPW
(ate_ipw <- mean(w_treated * base_test$Y - w_control * base_test$Y))
```


## AIPW

The **Augmented Inverse Propensity Weighting** (AIPW) estimator for the ATE writes:
$$
\widehat{\text{ATE}}_{\text{AIPW}} = \frac{1}{n} \sum_{i=1}^n \left[ \frac{T_i (Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} - \frac{(1 - T_i)(Y_i - \hat{\mu}_0(X_i))}{1 - \hat{e}(X_i)} + \hat{\mu}_1(X_i) - \hat{\mu}_0(X_i) \right],
$$

where $e(X_i) = \mathbb{P}(T_i = 1 \mid \boldsymbol{X}_i)$ is the propensity score, and where $\mu_1(\boldsymbol{X}_i) = \mathbb{E}[Y_i \mid \boldsymbol{X}_i, T_i = 1]$, $\mu_0(\boldsymbol{X}_i) = \mathbb{E}[Y_i \mid \boldsymbol{X}_i, T_i = 0]$.

```{r}
# Outcome models: estimation of mu_0 and mu_1
mu1_model <- randomForest(
  x = X_train[ind_train_1, ],
  y = base_train$Y[ind_train_1]
)
mu0_model <- randomForest(
  x = X_train[ind_train_0, ],
  y = base_train$Y[ind_train_0]
)

mu1_hat <- predict(mu1_model, newdata = X_test)
mu0_hat <- predict(mu0_model, newdata = X_test)

# AIPW estimator
T_test <- base_test$T
Y_test <- base_test$Y

aipw_terms <- T_test * (Y_test - mu1_hat) / e_hat -
              (1 - T_test) * (Y_test - mu0_hat) / (1 - e_hat) +
              (mu1_hat - mu0_hat)

(ate_aipw <- mean(aipw_terms))
```


## Causal Forest {#sec-ate-cf}

A more flexible alternative is the Causal Forest estimator (@Wager_2018, @Athey_2019), which uses tree-based methods to non-parametrically estimate heterogeneous treatment effects. Given covariates $\boldsymbol{X}$, treatment $T$, and outcome $Y$, we first estimate a causal forest $\hat{\tau}(\boldsymbol{X})$ using the `causal_forest()`{.R} function from [{grf}](https://cran.r-project.org/web/packages/grf/index.html). We can estimate the ATE by averaging predicted individual effects:

$$
\widehat{\text{ATE}} = \frac{1}{n} \sum_{i=1}^n \hat{\tau}(X_i).
$$

More robustly, we can rely on the forest's internal doubly robust estimator (@Lee_2017) via `average_treatment_effect()`{.R} in the {grf} package. The forest is trained by constructing an ensemble of trees, each grown on a subsample of the data using "honest" splits: one part of the sample determines the tree structure, and the other is used to estimate treatment effects at the leaves.

```{r define-ate_cf}
library(grf)
fit_cf <- causal_forest(
  X = X_train, 
  Y = base_train$Y, 
  W = base_train$T
)

(ate_cf <- average_treatment_effect(fit_cf, target.sample = "all"))
```


# CATE


The **Conditional Average Treatment Effect** (CATE) is defined as

$$
\tau(\boldsymbol{x}) = \mathbb{E}[Y(1) - Y(0) \mid \boldsymbol{X} = \boldsymbol{x}].
$$


## AIPW

The AIPW for the **ATE** is given by:

$$
\hat{\tau}(\boldsymbol{X}_i) = \frac{T_i (Y_i - \hat{\mu}_1(\boldsymbol{X}_i))}{\hat{e}(\boldsymbol{X}_i)} - \frac{(1 - T_i)(Y_i - \hat{\mu}_0(\boldsymbol{X}_i))}{1 - \hat{e}(\boldsymbol{X}_i)} + \hat{\mu}_1(\boldsymbol{X}_i) - \hat{\mu}_0(\boldsymbol{X}_i)
$$

```{r cate_aipw}
cate_aipw <- T * (Y - mu1_hat) / e_hat -
             (1 - T) * (Y - mu0_hat) / (1 - e_hat) +
             (mu1_hat - mu0_hat)

head(cate_aipw)
```


## X-Learner {#sec-cate-x-learner}

We use the **X-Learner** algorithm with Random Forests (@Kunzel_2019). The procedure consists of four steps:

1. We estimate the conditional outcome functions $\mu_1(\boldsymbol{x}) = \mathbb{E}[Y \mid T = 1, \boldsymbol{X} = \boldsymbol{x}]$ and $\mu_0(\boldsymbol{x}) = \mathbb{E}[Y \mid T = 0, X = \boldsymbol{x}]$ by fitting separate regression models on the treated and control groups, respectively. 
2. Then, we construct **pseudo-outcomes** that approximate the unobserved treatment effects: for treated individuals, we define $D_1 = Y - \hat{\mu}_0(\boldsymbol{X})$, and for controls, $D_0 = \hat{\mu}_1(\boldsymbol{X}) - Y$. These pseudo-outcomes are used to train two separate models, $\hat{\tau}_1(x)$ and $\hat{\tau}_0(x)$, which estimate treatment effects in the treated and control subpopulations. 
3. Then, we estimate the **propensity score** $g(\boldsymbol{x}) = \mathbb{P}(T = 1 \mid \boldsymbol{X} = \boldsymbol{x})$ using logistic regression on the training data.
4. Finally, the CATE for each individual in the test set is estimated via a weighted combination of the two learned functions:\
$$
\hat{\tau}(\boldsymbol{x}) = g(\boldsymbol{x}) \cdot \hat{\tau}_0(\boldsymbol{x}) + (1 - g(\boldsymbol{x})) \cdot \hat{\tau}_1(\boldsymbol{x}),
$$
which gives more weight to the group an individual is likely to belong to.

```{r cate-x-learner}
# Treatment effects:
# Treated: T_i = Y_i - \mu_0(X_i)
# mu0_hat_test <- predict(mu0_model, newdata = X_test) # potential outcome on test
# pseudo treatment effects
D1 <- base_train$Y[ind_train_1] - 
  predict(mu0_model, newdata = X_train[ind_train_1, ])

# Control: T_i = \mu_1(X_i) - Y_i
# mu1_hat_test <- predict(mu1_model, newdata = X_test) # potential outcome on test
D0 <- predict(mu1_model, newdata = X_train[ind_train_0, ]) -
  base_train$Y[ind_train_0]

# Train tau models:
tau1_model <- randomForest(x = X_train[ind_train_1, ], y = D1)
tau0_model <- randomForest(x = X_train[ind_train_0, ], y = D0)

# Propensity scores (on training set):
g_hat_test <- e_hat

# Ppseudo-effects on test set:
tau1_hat <- predict(tau1_model, newdata = X_test)
tau0_hat <- predict(tau0_model, newdata = X_test)
```

The estimated individual CATE for each observation in the test set:
```{r define-cate_xlearner}
cate_xlearner <- g_hat_test * tau0_hat + (1 - g_hat_test) * tau1_hat
head(cate_xlearner)
```

## Causal Forest {#sec-cate-cf}

We can use a causal forest. We use the forest trained in @sec-ate-cf and estimate the individual-level CATE estimaes $\hat{\tau}(X_i)$.

```{r define-cate_cf}
pred_cf <- predict(fit_cf, newdata = X_test)
cate_cf <- pred_cf$predictions
head(cate_cf)
```

# ATT

Let us now turn to the **Average Treatment Effect on the Treated (ATT)**, defined as

$$
\text{ATT} = \mathbb{E}[Y(1) - Y(0) \mid T = 1].
$$

## X-Learner

We compute the ATT from the **X-Learner** by averaging the predicted conditional treatment effects $\hat{\tau}(\boldsymbol{X}_i)$ over treated units in the test set:

$$
\widehat{\text{ATT}}_{\boldsymbol{X}} = \frac{1}{n_1} \sum_{i: T_i = 1} \hat{\tau}(\boldsymbol{X}_i),
$$

where $\hat{\tau}(\boldsymbol{X}_i)$ is obtained from the X-Learner algorithm (see @sec-cate-x-learner) and $n_1$ is the number of treated individuals.


```{r define-att_xlearner}
(att_xlearner <- mean(cate_xlearner[ind_test_1]))
```

## IPW

We can also use the **inverse propensity score weighting** (IPW): 

1. We first estimate the propensity score $\hat{e}(\boldsymbol{X}) = \mathbb{P}(T = 1 \mid \boldsymbol{X})$ via logistic regression on the training set.
2. We then compute ATT as the difference between the mean outcome in the treated group and the weighted mean outcome in the control group:\
$$
\widehat{\text{ATT}}_{\text{IPW}} = \mathbb{E}[Y \mid T = 1] - \mathbb{E}_w[Y \mid T = 0],
$$
where control outcomes are weighted by $\hat{w}_i = \hat{e}(\boldsymbol{X}_i) / (1 - \hat{e}(\boldsymbol{X}_i))$.

```{r define-att_ipw}
Y <- base_test$Y
T <- base_test$T

# Weights for controls (T = 0)
weights <- rep(1, length(T))
weights[T == 0] <- e_hat[ind_test_0] / (1 - e_hat[ind_test_0])

# ATT
treated_mean <- mean(Y[ind_test_1])
control_weighted_mean <- weighted.mean(Y[ind_test_0], w = weights[ind_test_0])

(att_ipw <- treated_mean - control_weighted_mean)
```


## AIPW

The AIPW estimator for the ATT writes:

$$
\widehat{\text{ATT}}_{\text{AIPW}} = \frac{1}{n_1} \sum_{i: T_i = 1} \left[ Y_i - \hat{\mu}_0(X_i) \right] + \frac{1}{n_1} \sum_{i: T_i = 0} \frac{\hat{e}(X_i)}{1 - \hat{e}(X_i)} \left[ Y_i - \hat{\mu}_0(X_i) \right]
$$

```{r}
# Shorter named for some components of the formula:
Y <- base_test$Y
T <- base_test$T
n1 <- sum(T == 1)

# First part of the formula:
# difference between treated and their predicted control outcomes
diff_1 <- mean(Y[ind_test_1] - mu0_hat[ind_test_1])

# Second part of the formula
# with a correction term from control group
correction_weights <- e_hat[ind_test_0] / (1 - e_hat[ind_test_0])
diff_0 <- mean(correction_weights * (Y[ind_test_0] - mu0_hat[ind_test_0]))

# ATT with AIPW estimator
(att_aipw <- diff_1 + diff_0)
```


## Causal Forest

Third, we use **Causal Forests**, again, to estimate the ATT directly. The forest is trained to estimate heterogeneous treatment effects (see @sec-ate-cf), and the (doubly robust estimate of the) ATT is obtained by setting `target.sample = "treated"`{.R} in the `average_treatment_effect()`{.R} function. 


```{r define-att_cf}
att_cf <- average_treatment_effect(fit_cf, target.sample = "treated")
att_cf
```



# CATT

The **Conditional Average Treatment effect on the Treated** (CATT) is defined as:

$$
\text{CATT} = \mathbb{E}[Y(1) - Y(0) \mid \boldsymbol{X} = \boldsymbol{x}, T = 1].
$$

It corresponds to the expected treatment effect for treated individuals, conditional on their covariates $\boldsymbol{X}$. Hence, compared to the ATT, it allows to capture effect heterogeneity within the treated population.

## AIPW

The **AIPW estimator** of the CATE, for each observation writes:

$$
\hat{\tau}(\boldsymbol{x}_i) = \frac{T_i (Y_i - \hat{\mu}_1(\boldsymbol{x}_i))}{\hat{e}(\boldsymbol{x}_i)} - \frac{(1 - T_i)(Y_i - \hat{\mu}_0(\boldsymbol{x}_i))}{1 - \hat{e}(\boldsymbol{x}_i)} + \hat{\mu}_1(\boldsymbol{x}_i) - \hat{\mu}_0(\boldsymbol{x}_i)
$$

```{r}
# AIPW CATE for each individual
Y <- base_test$Y
T <- base_test$T

cate_aipw <- T * (Y - mu1_hat) / e_hat -
             (1 - T) * (Y - mu0_hat) / (1 - e_hat) +
             (mu1_hat - mu0_hat)

head(cate_aipw)
```

Now, we can aggregate the individual CATE by levels of $X_2$, for example.
```{r}
base_test |> 
  mutate(cate = cate_aipw) |> 
  filter(T == 1) |> 
  group_by(X2) |> 
  summarise(
    CATT = mean(cate), 
    n = n()
  )
```



## X-Learner

In @sec-cate-x-learner, we computed $\hat{\tau}(\boldsymbol{X}_i)$, for all test individuals
```{r show-cate_xlearner}
head(cate_xlearner)
```

We compute CATT for treated subgroups. Let us start with $X_1$, which is continuous. We can focus on quantiles of $X_1$, the quartiles for example.
```{r}
base_test |> 
  mutate(
    cate = cate_xlearner
  ) |> 
  filter(T == 1) |> 
  mutate(
    X1_bin = ntile(X1, 4) # 4 bins
  ) |> 
  group_by(X1_bin) |> 
  summarise(
    CATT = mean(cate), 
    n = n()
  )
```
For $X_2$, which is categorical


```{r}
base_test |> 
  mutate(cate = cate_xlearner) |> 
  filter(T == 1) |> 
  group_by(X2) |> 
  summarise(
    CATT = mean(cate), 
    n = n()
  )
```

## Causal Forest

In @sec-cate-cf, we computed the CATE using a causal forest.
```{r show-cate_cf}
head(cate_cf)
```

```{r}
base_test |> 
  mutate(cate = cate_cf) |> 
  filter(T == 1) |> 
  group_by(X2) |> 
  summarise(
    CATT = mean(cate), 
    n = n()
  )
```



# References {.unnumbered}
